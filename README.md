# <img src="stood-icon.png" width="80" height="80" style="vertical-align: middle;"> Stood Agent Library

The Stood Agent library is an agent framework that lets Large Language Models(LLMs) autonomously execute tasks while providing developers with control and monitoring capabilities.

This Rust-based AI multi-agent framework with multi-model support is an implementation inspired by AWS' Strands Agent library. The project incorporates key architectural concepts from Strands Agent while introducing its own variations ‚Äî it reinterprets the core design principles without aiming for complete feature parity or serving as a direct replacement.


## ‚ö†Ô∏è Important Notice

This project is neither supported by nor an official AWS project. It is an independent implementation and is provided as-is. **This software is distributed in an Alpha state, and its functionality may be incomplete or subject to change.**

## Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
stood = { git = "https://github.com/fibanez/stood.git" }
```

## Documentation

For comprehensive documentation, examples, and guides, see the [Documentation](docs/README.md). For core API reference, see the [API Documentation](docs/api.md).

## Overview

Stood is an AI multi-agent framework that provides:

- **Multi-Model Support** - Support for Claude, Nova, and Providers (work in progress)
- **Type-Safe Tools** - Compile-time validation of tool parameters with Rust's type system
- **Agentic Execution** - Agents can autonomously chain tools and make decisions to complete complex tasks
- **Enterprise Features** - Comprehensive error handling, observability, and performance optimization, with optional zero-configuration telemetry

## Key Features

### Core Components
- **Agent Module** - Core agent implementation with conversation management
- **Tools System** - Unified tool system with compile-time validation and MCP integration
- **Built-in Tools** - Ready-to-use tools including Think tool for structured problem-solving, calculator, file operations, HTTP requests, and system utilities
- **Built-in Evaluator Support** - Force evaluation at end of agent cycle with task evaluation, agent-based evaluation, and multi-perspective strategies
- **Multi-Model Client** - Native integration with AWS Bedrock and other providers
- **OpenTelemetry Integration** - Enterprise-grade observability with OTEL standards
- **MCP Support** - Model Context Protocol integration for external tools

### Why Rust for Agentic Work?

*   **Rust First**: A native Rust library that brings agentic functionality directly to Rust applications without external dependencies.
*   **Small Executables:** Rust compiles to highly optimized binaries, resulting in small executable sizes. This is particularly advantageous for constrained environments like small devices, Lambda and Lambda@Edge Functions, and containers, leading to faster startup times and reduced resource consumption ‚Äì translating to improved performance and lower costs in cloud deployments.
*   **Memory Safety:**  Rust's ownership system eliminates common memory-related errors at compile time, enhancing the reliability and security of agent operations.
*   **Concurrency & Parallelism:** Rust‚Äôs robust concurrency primitives facilitate efficient tool calling and parallel processing, enabling complex workflows to execute with minimal overhead.
*   **Exponential Scalability**: Multi-agent systems consume 15x more resources than single agents [(1)](https://www.anthropic.com/engineering/built-multi-agent-research-system), but Rust deployments achieve 60-80% memory reduction and 3-50x performance improvements over Python equivalents [(2)](https://www.linkedin.com/pulse/python-productivity-rust-performance-match-made-heaven-nilay-parikh-c08zf), making Rust essential for managing hundreds of specialized agents at scale with long-running processes.
*   **Network-First Architecture**: As Model Context Protocol (MCP) evolves from local to remote servers [(3)](https://www.anthropic.com/news/integrations), Rust's measured networking performance becomes a key differentiator. With Tokio's async runtime and zero-copy capabilities, Rust efficiently handles the latency challenges of RPC-based architectures while maintaining performance necessary for real-time agent interactions. 

## Quick Start

```rust
use stood::agent::Agent;        // Core agent builder and execution
use stood::tool;                // Macro for creating custom tools

#[tool]
/// Calculate the result of a mathematical expression
async fn calculate(expression: String) -> Result<f64, String> {
    // TODO: Implement expression parsing
    // For demonstration, returning a fixed value
    Ok(42.0)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create an agent with default settings (uses Claude 3.5 Haiku)
    let agent = Agent::builder()
        .tools(vec![calculate()])
        .build()
        .await?;

    // Execute agentic workflow
    let response = agent.execute("Calculate 25 * 17").await?;
    println!("{}", response);

    Ok(())
}
```

### With Custom Configuration

```rust
use stood::agent::Agent;                    // Core agent builder and execution
use stood::llm::models::Bedrock;           // AWS Bedrock model types
use stood::mcp::{MCPClient, MCPClientConfig}; // MCP client for external tools
use stood::mcp::transport::{TransportFactory, WebSocketConfig}; // MCP transport layer
use stood::tool;                           // Macro for creating custom tools

#[tool]
/// Get weather information for a location
async fn get_weather(location: String) -> Result<String, String> {
    // TODO: Implement weather API call
    // For demonstration, returning a fixed value
    Ok(format!("Sunny, 75¬∞F in {}", location))
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configure connection to remote MCP server
    let mcp_config = WebSocketConfig {
        url: "wss://mcp-tools.acme.com/api".to_string(),
        connect_timeout_ms: 10_000,
        ..Default::default()
    };
    // Configure MCP Client and connect
    let mut mcp_client = MCPClient::new(
        MCPClientConfig::default(), 
        TransportFactory::websocket(mcp_config)
    );
    mcp_client.connect().await?;

    // Create an agent with custom configuration and remote MCP tools
    let agent = Agent::builder()
        .model(Bedrock::ClaudeHaiku45)
        .system_prompt("You are a helpful assistant with access to Acme Corp tools")
        .temperature(0.7)
        .max_tokens(1000)
        .tools(vec![get_weather()])
        .with_mcp_client(mcp_client, Some("acme_".to_string())).await?
        .build()
        .await?;

    let response = agent.execute("What's the weather like in San Francisco? and What are the latest news from Acme?").await?;
    println!("{}", response);

    Ok(())
}
```

## Autonomous Agent Evaluation

Stood includes an **optional LLM-controlled continuation system** that enables autonomous multi-cycle agents. **By default, agents execute one cycle and let the model decide naturally when it wants to continue.** You can enable explicit evaluation strategies for more complex continuation logic.

### Evaluation Strategies

| Strategy              | Mode  | Description                                                                                                                                    | Use Case                                                          |
|-----------------------|-------|------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|
| **Model-Driven**      | model | **Defaults** - No explicit evaluation, model decides continuation naturally (single-cycle). Developer can use Think tool to enhance this mode. | Natural interaction, simple tasks, letting the model control flow |
| **Task Evaluation**   | task  | Agent evaluates user intent satisfaction and task completion (multi-cycle)                                                                     | Most tasks, quality assurance, iterative refinement               |
| **Agent-Based**       | agent | Separate evaluator agent independently assesses work quality and completion                                                                    | Code review, complex evaluation logic, specialized domains        |
| **Multi-Perspective** | multi | Multiple evaluation viewpoints with weighted scoring for comprehensive assessment                                                              | Content creation, comprehensive analysis, balanced evaluation     |

**Note:** Self-reflection and chain-of-thought patterns can be achieved using **Task Evaluation** with custom prompts (e.g., "Let me think step by step..." or "Reflect on the quality of my work...").

### Evaluation Examples

```rust
// Model-Driven (DEFAULT) - Single-cycle execution, model decides naturally
let agent = Agent::builder()
    // Optionally use the think tool for LLM-directed evaluation of next steps
    .with_think_tool("Use the tool to think about something. It will not obtain new information, but just append the thought to the log. Use it when complex reasoning is needed.")
    .build().await?;  // Uses model-driven by default!

// Task Evaluation - Multi-cycle execution with user intent focus
let agent = Agent::builder()
    // Forces an extra evalution with the provided prompt when end of cycle is reached
    .with_task_evaluation("Have I fully satisfied the user's request? What aspects could be improved?")
    .build().await?;

// Task Evaluation with chain-of-thought style
let agent = Agent::builder()
    // Forces an additional evalution with the provided prompt when end of cycle is reached
    .with_task_evaluation("Let me think step by step: 1) What did the user ask for? 2) What have I accomplished? 3) What's missing? 4) Is the task complete?")
    .build().await?;

// Agent-based evaluation with specialized evaluator
// First create the evaluator agent
let evaluator = Agent::builder()
    .model(Bedrock::ClaudeHaiku45)
    .system_prompt("You are a critical evaluator. Assess task completion quality.")
    .build().await?;

// Then attach the evaluator to the main agent
let main_agent = Agent::builder()
    .with_agent_based_evaluation(evaluator)
    .build().await?;

// Multi-perspective evaluation with weighted scoring
// First create the multi-perspective evaluation configuration
let perspectives = vec![
    PerspectiveConfig {
        name: "quality_check".to_string(),
        prompt: "As a quality analyst, is the work complete and high-quality?".to_string(),
        weight: 0.6,
    },
    PerspectiveConfig {
        name: "user_satisfaction".to_string(),
        prompt: "From a user's perspective, does this fully address their needs?".to_string(),
        weight: 0.4,
    },
];
// Then attach the perspective configuration to the main agent  
let agent = Agent::builder()
    .with_multi_perspective_evaluation(perspectives)
    .build().await?;
```

### Benefits

- **Natural by Default**: Model-driven execution lets the model decide continuation naturally
- **Optional Multi-Cycle**: Enable explicit evaluation strategies when you need multi-cycle behavior
- **User-Focused**: Evaluation strategies center on completing the user's actual request, not arbitrary metrics
- **Flexible Architecture**: Choose from model-driven, task evaluation, agent-based, or multi-perspective strategies
- **Simple Configuration**: Clear API with explicit opt-in for evaluation complexity
- **Observable**: Full telemetry and logging of evaluation decisions and reasoning


## Examples

The examples are organized by complexity:

### Basic Examples
- [001\_tool\_macro](examples/001_tool_macro.rs) - Custom tools with #[tool] macro
- [002\_tool\_decorator\_registry](examples/002_tool_decorator_registry.rs) - Tool decorator with registry
- [003\_interactive\_chat\_simple](examples/003_interactive_chat_simple.rs) - Simple interactive chat
- [004\_streaming\_simple](examples/004_streaming_simple.rs) - Simple streaming response handling
- [005\_callbacks\_basic](examples/005_callbacks_basic.rs) - Basic callback patterns
- [006\_callback\_system\_demo](examples/006_callback_system_demo.rs) - Callback system integration
- [007\_debug\_logging](examples/007_debug_logging.rs) - Debug logging configuration

### Intermediate Examples
- [008\_streaming\_custom\_callbacks](examples/008_streaming_custom_callbacks.rs) - Custom streaming callbacks
- [009\_logging\_demo](examples/009_logging_demo.rs) - Performance logging setup and configuration
- [010\_streaming\_with\_tools](examples/010_streaming_with_tools.rs) - Streaming with tool integration
- [011\_basic\_agent](examples/011_basic_agent.rs) - Basic agent with multiple provider support
- [012\_batching\_optimization\_demo](examples/012_batching_optimization_demo.rs) - Batching optimization patterns
- [013\_mcp\_integration](examples/013_mcp_integration.rs) - Simple MCP server integration
- [014\_mcp\_configuration\_examples](examples/014_mcp_configuration_examples.rs) - MCP configuration examples
- [015\_authorization\_chat\_wrapper](examples/015_authorization_chat_wrapper.rs) - Authorization patterns

### Advanced Examples
- [016\_context\_management](examples/016_context_management.rs) - Context management patterns
- [017\_parallel\_execution](examples/017_parallel_execution.rs) - Parallel execution patterns
- [018\_task\_evaluation](examples/018_task_evaluation.rs) - Task evaluation strategy (default multi-cycle behavior)
- [019\_agent\_based\_evaluation](examples/019_agent_based_evaluation.rs) - Agent-based evaluation example
- [020\_multi\_perspective](examples/020_multi_perspective.rs) - Multi-perspective evaluation example
- [021\_agentic\_chat](examples/021_agentic_chat.rs) - Full interactive chat application

### Expert Examples
- [022\_aws\_doc\_mcp](examples/022_aws_doc_mcp/) - AWS documentation MCP integration
- [023\_telemetry](examples/023_telemetry/) - Comprehensive telemetry examples

## TODO - Work in Progress

‚ö†Ô∏è **This project is currently in active development and many features are planned but not yet implemented.**

### Provider Support Status

**‚úÖ Implemented Providers:**
- AWS Bedrock (Claude 3.5 Haiku, Nova, other Bedrock models)
- LM Studio (local development and testing)

**üöß Planned Providers (Not Yet Implemented):**
- Anthropic API (direct Claude API access)
- OpenAI (GPT-4, GPT-3.5)
- Ollama (local LLM hosting)
- OpenRouter (multi-provider proxy)
- Candle (local Rust-based inference)

### Major Features Planned

**üéØ High Priority:**
- [ ] **Dynamic Model Registration** - runtime model registration using string-based identification patterns
- [ ] **AWS AgentCore Framework Integration** - Full compatibility with AWS AgentCore for enterprise deployments
- [ ] **Complete Provider Implementation** - Finish all placeholder providers (Anthropic, OpenAI, Ollama, OpenRouter, Candle)
- [ ] **Enhanced MCP Support** - Expand Model Context Protocol integration with more server types - AUTH
- [ ] A2A Protocol Support - Implement standard for Agent to Agent communication
- [ ] **Production Hardening** - Enhanced error handling, retry logic, and resilience patterns


## License

Licensed under Apache License, Version 2.0



