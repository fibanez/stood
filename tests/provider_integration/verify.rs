//! Simplified verification runner - single entry point for all tests
//!
//! Usage:
//! cargo run --bin verify                           # All tests, all providers
//! cargo run --bin verify -- core                  # Core tests, all providers  
//! cargo run --bin verify -- core --provider bedrock # Core tests, Bedrock only
//! cargo run --bin verify -- --provider lm_studio  # All tests, LM Studio only
//! cargo run --bin verify -- --test builtin_file_read --provider bedrock # Single test, specific provider
//! cargo run --bin verify -- --test tool_registry  # Single test, all providers
//! cargo run --bin verify -- --model claude-3-5-haiku # Model-specific tests across providers
//! cargo run --bin verify -- tools --debug         # Enable debug output

use clap::{Arg, Command};
use futures::future::join_all;
use std::sync::Arc;
use tokio::sync::Semaphore;
use std::collections::HashMap;
use std::time::Instant;

/// Test filtering options for granular test selection
#[derive(Debug, Clone)]
pub struct TestFilters {
    pub suites: Option<Vec<TestSuite>>,
    pub providers: Option<Vec<Provider>>,
    pub models: Option<Vec<String>>,
    pub test_names: Option<Vec<String>>,
    pub debug: bool,
}

impl TestFilters {
    /// Check if a test case matches the filters
    pub fn matches(&self, test_case: &TestCase) -> bool {
        // Check suite filter
        if let Some(suites) = &self.suites {
            if !suites.contains(&test_case.suite) {
                return false;
            }
        }
        
        // Check provider filter
        if let Some(providers) = &self.providers {
            if !providers.contains(&test_case.provider) {
                return false;
            }
        }
        
        // Check model filter
        if let Some(models) = &self.models {
            if !models.contains(&test_case.model_name) {
                return false;
            }
        }
        
        // Check test name filter
        if let Some(test_names) = &self.test_names {
            if !test_names.contains(&test_case.test_name) {
                return false;
            }
        }
        
        true
    }
    
    /// Create default filters (no filtering)
    pub fn new() -> Self {
        Self {
            suites: None,
            providers: None,
            models: None,
            test_names: None,
            debug: false,
        }
    }
}

/// Test suite types (English names instead of milestones)
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum TestSuite {
    Core,          // Basic functionality (was Milestone 1)
    Tools,         // Tool integration (was Milestone 2) 
    Streaming,     // Streaming features (was Milestone 3)
    TokenCounting, // Token counting verification (Telemetry)
    Advanced,      // Advanced features (was Milestone 4+)
}

impl TestSuite {
    fn from_str(s: &str) -> Option<Self> {
        match s.to_lowercase().as_str() {
            "core" => Some(Self::Core),
            "tools" => Some(Self::Tools),
            "streaming" => Some(Self::Streaming),
            "token_counting" | "tokens" | "telemetry" => Some(Self::TokenCounting),
            "advanced" => Some(Self::Advanced),
            _ => None,
        }
    }
    
    fn as_str(&self) -> &'static str {
        match self {
            Self::Core => "core",
            Self::Tools => "tools", 
            Self::Streaming => "streaming",
            Self::TokenCounting => "token_counting",
            Self::Advanced => "advanced",
        }
    }
}

/// Provider types for verification
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum Provider {
    LmStudio,
    Bedrock,
}

impl Provider {
    fn from_str(s: &str) -> Option<Self> {
        match s.to_lowercase().as_str() {
            "lm_studio" | "lm-studio" | "lmstudio" => Some(Self::LmStudio),
            "bedrock" | "haiku" => Some(Self::Bedrock),
            _ => None,
        }
    }
    
    #[allow(dead_code)]
    fn as_str(&self) -> &'static str {
        match self {
            Self::LmStudio => "lm_studio",
            Self::Bedrock => "bedrock",
        }
    }
    
    fn display_name(&self) -> &'static str {
        match self {
            Self::LmStudio => "LM Studio",
            Self::Bedrock => "Bedrock", 
        }
    }
}

/// Test result for a single test case
#[derive(Debug, Clone)]
pub struct TestResult {
    pub provider: Provider,
    pub suite: TestSuite,
    pub test_name: String,
    pub model_name: String,
    pub passed: bool,
    pub duration_ms: u64,
    pub error: Option<String>,
}

/// Aggregated results for a provider/suite combination
#[derive(Debug, Clone)]
pub struct SuiteResult {
    pub provider: Provider,
    pub suite: TestSuite,
    pub passed: usize,
    pub total: usize,
    pub duration_ms: u64,
    pub errors: Vec<String>,
}

impl SuiteResult {
    pub fn success_rate(&self) -> f64 {
        if self.total == 0 { 0.0 } else { (self.passed as f64 / self.total as f64) * 100.0 }
    }
    
    pub fn is_success(&self) -> bool {
        self.passed == self.total
    }
}

impl TestResult {
    pub fn is_success(&self) -> bool {
        self.passed
    }
}

/// Individual test case definition
#[derive(Debug, Clone)]
pub struct TestCase {
    pub provider: Provider,
    pub suite: TestSuite,
    pub test_name: String,
    pub model_name: String,
    pub test_id: TestId,
}

/// Execution strategy for different providers
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ExecutionStrategy {
    /// All tests run in parallel (default for cloud providers)
    FullParallel,
    /// Sequential by model, parallel within model (for local providers like LM Studio)
    SequentialByModel,
    /// Completely sequential (for debugging)
    FullSequential,
}

/// Group of test cases for a specific model
#[derive(Debug, Clone)]
pub struct ModelGroup {
    pub provider: Provider,
    pub model_name: String,
    pub test_cases: Vec<TestCase>,
    pub execution_strategy: ExecutionStrategy,
}

/// Complete execution plan organized by provider and model
#[derive(Debug, Clone)]
pub struct ExecutionPlan {
    pub bedrock_groups: Vec<ModelGroup>,     // Can run fully parallel
    pub lm_studio_groups: Vec<ModelGroup>,   // Must run sequentially by model
    pub other_provider_groups: Vec<ModelGroup>,
}

impl ExecutionPlan {
    pub fn total_test_count(&self) -> usize {
        let bedrock_count: usize = self.bedrock_groups.iter().map(|g| g.test_cases.len()).sum();
        let lm_studio_count: usize = self.lm_studio_groups.iter().map(|g| g.test_cases.len()).sum();
        let other_count: usize = self.other_provider_groups.iter().map(|g| g.test_cases.len()).sum();
        bedrock_count + lm_studio_count + other_count
    }
    
    pub fn is_empty(&self) -> bool {
        self.bedrock_groups.is_empty() && 
        self.lm_studio_groups.is_empty() && 
        self.other_provider_groups.is_empty()
    }
    
    /// Generate an execution plan from test cases
    pub fn from_test_cases(test_cases: Vec<TestCase>) -> Self {
        use std::collections::HashMap;
        
        let mut bedrock_models: HashMap<String, Vec<TestCase>> = HashMap::new();
        let mut lm_studio_models: HashMap<String, Vec<TestCase>> = HashMap::new();
        let mut other_models: HashMap<String, Vec<TestCase>> = HashMap::new();
        
        // Group test cases by provider and model
        for test_case in test_cases {
            match test_case.provider {
                Provider::Bedrock => {
                    bedrock_models.entry(test_case.model_name.clone())
                        .or_insert_with(Vec::new)
                        .push(test_case);
                },
                Provider::LmStudio => {
                    lm_studio_models.entry(test_case.model_name.clone())
                        .or_insert_with(Vec::new)
                        .push(test_case);
                },
            }
        }
        
        // Create model groups with appropriate execution strategies
        let bedrock_groups: Vec<ModelGroup> = bedrock_models.into_iter()
            .map(|(model_name, test_cases)| ModelGroup {
                provider: Provider::Bedrock,
                model_name,
                test_cases,
                execution_strategy: ExecutionStrategy::FullParallel,
            })
            .collect();
            
        let lm_studio_groups: Vec<ModelGroup> = lm_studio_models.into_iter()
            .map(|(model_name, test_cases)| ModelGroup {
                provider: Provider::LmStudio,
                model_name,
                test_cases,
                execution_strategy: ExecutionStrategy::SequentialByModel,
            })
            .collect();
            
        let other_provider_groups: Vec<ModelGroup> = other_models.into_iter()
            .map(|(model_name, test_cases)| ModelGroup {
                provider: test_cases[0].provider.clone(), // Assume all same provider
                model_name,
                test_cases,
                execution_strategy: ExecutionStrategy::FullParallel,
            })
            .collect();
        
        Self {
            bedrock_groups,
            lm_studio_groups,
            other_provider_groups,
        }
    }
}

/// Test function identifier
#[derive(Debug, Clone, PartialEq)]
pub enum TestId {
    // Core functionality tests
    LmStudioBasicChat,
    LmStudioMultiTurn,
    LmStudioHealthCheck,
    LmStudioCapabilities,
    LmStudioConfiguration,
    BedrockBasicChat,
    BedrockMultiTurn,
    BedrockHealthCheck,
    BedrockCapabilities,
    BedrockConfiguration,
    // Provider registry tests
    ProviderRegistryLmStudio,
    ProviderRegistryBedrock,
    // Error scenario tests
    LmStudioUnavailable,
    InvalidConfiguration,
    // Agent builder tests
    AgentBuilderComplete,
    AgentBuilderDefaults,
    // Tool system tests (Milestone 2)
    LmStudioToolRegistry,
    LmStudioToolBuiltinCalculator,
    LmStudioToolBuiltinFileRead,
    LmStudioToolCustomMacro,
    LmStudioToolParallelExecution,
    BedrockToolRegistry,
    BedrockToolBuiltinCalculator,
    BedrockToolBuiltinFileRead,
    BedrockToolCustomMacro,
    BedrockToolParallelExecution,
    // Nova Micro tests (Core)
    NovaBasicChat,
    NovaMultiTurn,
    NovaHealthCheck,
    NovaCapabilities,
    NovaConfiguration,
    NovaProviderRegistry,
    // Nova Micro tool tests
    NovaToolRegistry,
    NovaToolBuiltinCalculator,
    NovaToolBuiltinFileRead,
    NovaToolCustomMacro,
    NovaToolParallelExecution,
    // Streaming tests (Milestone 3)
    LmStudioBasicStreaming,
    LmStudioStreamingWithTools,
    BedrockBasicStreaming,
    BedrockStreamingWithTools,
    NovaBasicStreaming,
    NovaStreamingWithTools,
    // Token counting tests (Telemetry)
    LmStudioStreamingTokenCounting,
    LmStudioNonStreamingTokenCounting,
    LmStudioStreamingTokenCountingWithTools,
    LmStudioTokenCountingConsistency,
    ClaudeStreamingTokenCounting,
    ClaudeNonStreamingTokenCounting,
    ClaudeStreamingTokenCountingWithTools,
    ClaudeTokenCountingConsistency,
    NovaStreamingTokenCounting,
    NovaNonStreamingTokenCounting,
    NovaStreamingTokenCountingWithTools,
    NovaTokenCountingConsistency,
}

/// Parallel verification runner
#[derive(Debug)]
pub struct VerificationRunner {
    results: Vec<TestResult>,
    max_parallel: usize,
}

impl VerificationRunner {
    pub fn new() -> Self {
        let max_parallel = std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(4); // Fallback to 4 if detection fails
        
        println!("🚀 Detected {} CPU cores, running tests with {} parallel workers", max_parallel, max_parallel);
        
        Self { 
            results: Vec::new(),
            max_parallel,
        }
    }
    
    /// Run verification tests using model-aware execution strategy
    pub async fn run(&mut self, filters: TestFilters) -> Result<(), Box<dyn std::error::Error>> {
        if filters.debug {
            println!("🔍 Debug mode enabled");
            println!("🔍 Filters: {:#?}", filters);
        }
        
        println!("🧪 Running verification tests with model-aware execution");
        
        // Build all test cases with filtering
        let test_cases = self.build_test_cases(&filters).await?;
        println!("📊 Total test cases: {}", test_cases.len());
        
        // Generate execution plan
        let execution_plan = ExecutionPlan::from_test_cases(test_cases);
        
        if execution_plan.is_empty() {
            println!("⚠️  No test cases match the provided filters");
            return Ok(());
        }
        
        self.print_execution_plan_summary(&execution_plan);
        
        let mut all_results = Vec::new();
        
        // Execute Bedrock tests (fully parallel)
        if !execution_plan.bedrock_groups.is_empty() {
            println!("\n🚀 Executing Bedrock tests (fully parallel)");
            let bedrock_results = self.execute_parallel_groups(&execution_plan.bedrock_groups).await;
            all_results.extend(bedrock_results);
        }
        
        // Execute LM Studio tests (sequential by model, parallel within model)
        if !execution_plan.lm_studio_groups.is_empty() {
            println!("\n🎯 Executing LM Studio tests (sequential by model)");
            let lm_studio_results = self.execute_sequential_by_model(&execution_plan.lm_studio_groups).await;
            all_results.extend(lm_studio_results);
        }
        
        // Execute other provider tests (fully parallel)
        if !execution_plan.other_provider_groups.is_empty() {
            println!("\n⚡ Executing other provider tests (fully parallel)");
            let other_results = self.execute_parallel_groups(&execution_plan.other_provider_groups).await;
            all_results.extend(other_results);
        }
        
        // Store results and print summary
        self.results = all_results;
        self.print_summary();
        Ok(())
    }
    
    /// Print summary of the execution plan
    fn print_execution_plan_summary(&self, plan: &ExecutionPlan) {
        println!("\n📋 Execution Plan Summary:");
        
        if !plan.bedrock_groups.is_empty() {
            let bedrock_count: usize = plan.bedrock_groups.iter().map(|g| g.test_cases.len()).sum();
            println!("  🚀 Bedrock: {} tests across {} models (fully parallel)", 
                bedrock_count, plan.bedrock_groups.len());
        }
        
        if !plan.lm_studio_groups.is_empty() {
            let lm_studio_count: usize = plan.lm_studio_groups.iter().map(|g| g.test_cases.len()).sum();
            println!("  🎯 LM Studio: {} tests across {} models (sequential by model)", 
                lm_studio_count, plan.lm_studio_groups.len());
        }
        
        if !plan.other_provider_groups.is_empty() {
            let other_count: usize = plan.other_provider_groups.iter().map(|g| g.test_cases.len()).sum();
            println!("  ⚡ Other: {} tests across {} models (fully parallel)", 
                other_count, plan.other_provider_groups.len());
        }
        
        println!("  📈 Total: {} tests", plan.total_test_count());
    }
    
    /// Execute multiple model groups in parallel (for Bedrock and other cloud providers)
    async fn execute_parallel_groups(&self, groups: &[ModelGroup]) -> Vec<TestResult> {
        let semaphore = Arc::new(Semaphore::new(self.max_parallel));
        
        let mut all_futures = Vec::new();
        for group in groups {
            for test_case in &group.test_cases {
                let sem = semaphore.clone();
                let test_case = test_case.clone();
                
                let future = self.execute_single_test_with_semaphore(sem, test_case);
                
                all_futures.push(future);
            }
        }
        
        join_all(all_futures).await
    }
    
    /// Execute model groups sequentially by model, parallel within each model (for LM Studio)
    /// Uses Prime-then-Parallel strategy to avoid model loading race conditions
    async fn execute_sequential_by_model(&self, groups: &[ModelGroup]) -> Vec<TestResult> {
        let mut all_results = Vec::new();
        
        for (i, group) in groups.iter().enumerate() {
            println!("\n🔄 Loading model {}/{}: {} (LM Studio)", 
                i + 1, groups.len(), group.model_name);
            
            if group.test_cases.is_empty() {
                continue;
            }
            
            // Prime-then-Parallel strategy for LM Studio:
            // 1. Run the first test alone to trigger model loading
            // 2. If successful, run remaining tests in parallel
            // This prevents 404 errors during concurrent model loading
            
            println!("🎯 Priming model with first test...");
            let first_test = &group.test_cases[0];
            let prime_result = self.execute_single_test(first_test.clone()).await;
            
            let mut model_results = vec![prime_result.clone()];
            
            if prime_result.passed {
                println!("✅ Model primed successfully, running remaining {} tests in parallel", 
                    group.test_cases.len() - 1);
                
                // Run remaining tests in parallel now that model is loaded
                if group.test_cases.len() > 1 {
                    let semaphore = Arc::new(Semaphore::new(self.max_parallel));
                    
                    let remaining_futures: Vec<_> = group.test_cases[1..].iter().map(|test_case| {
                        let sem = semaphore.clone();
                        let test_case = test_case.clone();
                        
                        self.execute_single_test_with_semaphore(sem, test_case)
                    }).collect();
                    
                    let remaining_results = join_all(remaining_futures).await;
                    model_results.extend(remaining_results);
                }
            } else {
                println!("❌ Model priming failed, skipping remaining tests for {}", group.model_name);
                println!("   Priming error: {}", prime_result.error.as_deref().unwrap_or("Unknown error"));
                
                // Mark remaining tests as failed due to model loading failure
                for test_case in &group.test_cases[1..] {
                    let failed_result = TestResult {
                        provider: test_case.provider.clone(),
                        suite: test_case.suite.clone(),
                        test_name: test_case.test_name.clone(),
                        model_name: test_case.model_name.clone(),
                        passed: false,
                        duration_ms: 0,
                        error: Some(format!("Model loading failed during priming: {}", 
                            prime_result.error.as_deref().unwrap_or("Unknown error"))),
                    };
                    model_results.push(failed_result);
                }
            }
            
            // Print model completion summary
            let passed = model_results.iter().filter(|r| r.passed).count();
            let total = model_results.len();
            let success_rate = (passed as f64 / total as f64) * 100.0;
            
            println!("✅ Model {} completed: {}/{} tests passed ({:.1}%)", 
                group.model_name, passed, total, success_rate);
            
            all_results.extend(model_results);
            
            // Small delay between models to allow LM Studio to stabilize
            if i < groups.len() - 1 {
                tokio::time::sleep(std::time::Duration::from_millis(500)).await;
            }
        }
        
        all_results
    }
    
    /// Execute a single test case and return the result
    async fn execute_single_test(&self, test_case: TestCase) -> TestResult {
        let start_time = Instant::now();
        
        // Create a temporary runner for each test (without printing CPU detection)
        let temp_runner = VerificationRunner { 
            results: Vec::new(), 
            max_parallel: 1 // Not used for individual test execution
        };
        
        let result = match temp_runner.execute_test(&test_case).await {
            Ok(_) => TestResult {
                provider: test_case.provider,
                suite: test_case.suite,
                test_name: test_case.test_name,
                model_name: test_case.model_name.clone(),
                passed: true,
                duration_ms: start_time.elapsed().as_millis() as u64,
                error: None,
            },
            Err(e) => TestResult {
                provider: test_case.provider,
                suite: test_case.suite,
                test_name: test_case.test_name,
                model_name: test_case.model_name.clone(),
                passed: false,
                duration_ms: start_time.elapsed().as_millis() as u64,
                error: Some(e.to_string()),
            },
        };
        
        // Print progress as tests complete with category first
        if result.passed {
            println!("✅ {}/{}/{}/{} ({:.1}s)", 
                result.suite.as_str(),
                result.test_name,
                result.provider.display_name(),
                result.model_name,
                result.duration_ms as f64 / 1000.0);
        } else {
            println!("❌ {}/{}/{}/{} ({:.1}s): {}", 
                result.suite.as_str(),
                result.test_name,
                result.provider.display_name(),
                result.model_name,
                result.duration_ms as f64 / 1000.0,
                result.error.as_ref().unwrap_or(&"Unknown error".to_string()));
        }
        
        result
    }
    
    /// Execute a single test case with semaphore coordination
    async fn execute_single_test_with_semaphore(&self, semaphore: Arc<Semaphore>, test_case: TestCase) -> TestResult {
        let _permit = semaphore.acquire().await.unwrap();
        self.execute_single_test(test_case).await
    }
    
    /// Build all test cases with filtering applied
    async fn build_test_cases(&self, filters: &TestFilters) -> Result<Vec<TestCase>, Box<dyn std::error::Error>> {
        let mut all_test_cases = Vec::new();
        
        // Determine which suites and providers to consider
        let suites = filters.suites.as_ref().map(|s| s.as_slice()).unwrap_or(&[TestSuite::Core, TestSuite::Tools, TestSuite::Streaming, TestSuite::TokenCounting, TestSuite::Advanced]);
        let providers = filters.providers.as_ref().map(|p| p.as_slice()).unwrap_or(&[Provider::LmStudio, Provider::Bedrock]);
        
        for suite in suites {
            for provider in providers {
                // Always add test cases - let individual tests fail with clear messages
                // instead of silently filtering them out
                
                // Add test cases based on suite and provider
                match (provider, suite) {
                    (Provider::LmStudio, TestSuite::Core) => {
                        // Generate core tests for all LM Studio models
                        let lm_studio_models = vec!["google/gemma-3-27b", "google/gemma-3-12b", "tessa-rust-t1-7b"];
                        
                        for model in lm_studio_models {
                            all_test_cases.extend(vec![
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "basic_chat".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioBasicChat,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "multi_turn".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioMultiTurn,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "health_check".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioHealthCheck,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "capabilities".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioCapabilities,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "configuration".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioConfiguration,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "provider_registry".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::ProviderRegistryLmStudio,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "error_unavailable".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioUnavailable,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "invalid_config".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::InvalidConfiguration,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "agent_builder_complete".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::AgentBuilderComplete,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "agent_builder_defaults".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::AgentBuilderDefaults,
                                },
                            ]);
                        }
                    }
                    (Provider::Bedrock, TestSuite::Core) => {
                        all_test_cases.extend(vec![
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "basic_chat".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockBasicChat,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "multi_turn".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockMultiTurn,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "health_check".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockHealthCheck,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "capabilities".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockCapabilities,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "configuration".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockConfiguration,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "provider_registry".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::ProviderRegistryBedrock,
                            },
                            // Nova Micro test cases (smallest/cheapest with tool support)
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "basic_chat".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaBasicChat,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "multi_turn".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaMultiTurn,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "health_check".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaHealthCheck,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "capabilities".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaCapabilities,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "configuration".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaConfiguration,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "provider_registry".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaProviderRegistry,
                            },
                        ]);
                    }
                    (Provider::LmStudio, TestSuite::Tools) => {
                        // Generate tool tests for all LM Studio models
                        let lm_studio_models = vec!["google/gemma-3-27b", "google/gemma-3-12b", "tessa-rust-t1-7b"];
                        
                        for model in lm_studio_models {
                            all_test_cases.extend(vec![
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "tool_registry".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioToolRegistry,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "builtin_calculator".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioToolBuiltinCalculator,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "builtin_file_read".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioToolBuiltinFileRead,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "custom_macro".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioToolCustomMacro,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "parallel_execution".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioToolParallelExecution,
                                },
                            ]);
                        }
                    }
                    (Provider::Bedrock, TestSuite::Tools) => {
                        all_test_cases.extend(vec![
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "tool_registry".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockToolRegistry,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "builtin_calculator".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockToolBuiltinCalculator,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "builtin_file_read".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockToolBuiltinFileRead,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "custom_macro".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockToolCustomMacro,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "parallel_execution".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockToolParallelExecution,
                            },
                            // Nova Micro tool test cases (smallest/cheapest with tool support)
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "tool_registry".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaToolRegistry,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "builtin_calculator".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaToolBuiltinCalculator,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "builtin_file_read".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaToolBuiltinFileRead,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "custom_macro".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaToolCustomMacro,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "parallel_execution".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaToolParallelExecution,
                            },
                        ]);
                    }
                    (Provider::LmStudio, TestSuite::Streaming) => {
                        // Generate streaming tests for all LM Studio models
                        let lm_studio_models = vec!["google/gemma-3-27b", "google/gemma-3-12b", "tessa-rust-t1-7b"];
                        
                        for model in lm_studio_models {
                            all_test_cases.extend(vec![
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "basic_streaming".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioBasicStreaming,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "streaming_with_tools".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioStreamingWithTools,
                                },
                            ]);
                        }
                    }
                    (Provider::Bedrock, TestSuite::Streaming) => {
                        all_test_cases.extend(vec![
                            // Claude 3.5 Haiku streaming tests
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "basic_streaming".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockBasicStreaming,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "streaming_with_tools".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::BedrockStreamingWithTools,
                            },
                            // Nova Micro streaming tests
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "basic_streaming".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaBasicStreaming,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "streaming_with_tools".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaStreamingWithTools,
                            },
                        ]);
                    }
                    (Provider::LmStudio, TestSuite::TokenCounting) => {
                        // Generate token counting tests for all LM Studio models
                        let lm_studio_models = vec!["google/gemma-3-27b", "google/gemma-3-12b", "tessa-rust-t1-7b"];
                        
                        for model in lm_studio_models {
                            all_test_cases.extend(vec![
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "streaming_token_counting".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioStreamingTokenCounting,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "non_streaming_token_counting".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioNonStreamingTokenCounting,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "streaming_token_counting_with_tools".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioStreamingTokenCountingWithTools,
                                },
                                TestCase {
                                    provider: provider.clone(),
                                    suite: suite.clone(),
                                    test_name: "token_counting_consistency".to_string(),
                                    model_name: model.to_string(),
                                    test_id: TestId::LmStudioTokenCountingConsistency,
                                },
                            ]);
                        }
                    }
                    (Provider::Bedrock, TestSuite::TokenCounting) => {
                        all_test_cases.extend(vec![
                            // Claude 3.5 Haiku token counting tests
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "streaming_token_counting".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::ClaudeStreamingTokenCounting,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "non_streaming_token_counting".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::ClaudeNonStreamingTokenCounting,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "streaming_token_counting_with_tools".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::ClaudeStreamingTokenCountingWithTools,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "token_counting_consistency".to_string(),
                                model_name: "claude-3-5-haiku".to_string(),
                                test_id: TestId::ClaudeTokenCountingConsistency,
                            },
                            // Nova Micro token counting tests
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "streaming_token_counting".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaStreamingTokenCounting,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "non_streaming_token_counting".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaNonStreamingTokenCounting,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "streaming_token_counting_with_tools".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaStreamingTokenCountingWithTools,
                            },
                            TestCase {
                                provider: provider.clone(),
                                suite: suite.clone(),
                                test_name: "token_counting_consistency".to_string(),
                                model_name: "amazon-nova-micro".to_string(),
                                test_id: TestId::NovaTokenCountingConsistency,
                            },
                        ]);
                    }
                    _ => {
                        // Placeholder for not-yet-implemented test suites
                    }
                }
            }
        }
        
        // Apply additional filtering based on test names and models
        let total_before_filtering = all_test_cases.len();
        let filtered_test_cases: Vec<TestCase> = all_test_cases
            .into_iter()
            .filter(|test_case| filters.matches(test_case))
            .collect();
        
        if filters.debug {
            println!("🔍 Debug: Generated {} test cases before filtering", total_before_filtering);
            println!("🔍 Debug: {} test cases after filtering", filtered_test_cases.len());
            for test_case in &filtered_test_cases {
                println!("🔍 Debug: Test case: {}/{}/{} [{}]", 
                    test_case.provider.display_name(),
                    test_case.suite.as_str(),
                    test_case.test_name,
                    test_case.model_name);
            }
        }
        
        Ok(filtered_test_cases)
    }
    
    /// Execute a specific test case
    async fn execute_test(&self, test_case: &TestCase) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        match test_case.test_id {
            TestId::LmStudioBasicChat => self.test_lm_studio_basic_chat(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioMultiTurn => self.test_lm_studio_multi_turn(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioHealthCheck => self.test_lm_studio_health_check().await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioCapabilities => self.test_lm_studio_capabilities().await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioConfiguration => self.test_lm_studio_configuration(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockBasicChat => self.test_bedrock_haiku_basic_chat().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockMultiTurn => self.test_bedrock_haiku_multi_turn().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockHealthCheck => self.test_bedrock_health_check().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockCapabilities => self.test_bedrock_capabilities().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockConfiguration => self.test_bedrock_configuration().await.map_err(|e| format!("{}", e).into()),
            // Provider registry tests
            TestId::ProviderRegistryLmStudio => self.test_provider_registry_lm_studio().await.map_err(|e| format!("{}", e).into()),
            TestId::ProviderRegistryBedrock => self.test_provider_registry_bedrock().await.map_err(|e| format!("{}", e).into()),
            // Error scenario tests
            TestId::LmStudioUnavailable => self.test_lm_studio_unavailable().await.map_err(|e| format!("{}", e).into()),
            TestId::InvalidConfiguration => self.test_invalid_configuration().await.map_err(|e| format!("{}", e).into()),
            // Agent builder tests
            TestId::AgentBuilderComplete => self.test_agent_builder_complete(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::AgentBuilderDefaults => self.test_agent_builder_defaults(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            // Tool system tests (Milestone 2)
            TestId::LmStudioToolRegistry => self.test_lm_studio_tool_registry(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioToolBuiltinCalculator => self.test_lm_studio_tool_builtin_calculator(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioToolBuiltinFileRead => self.test_lm_studio_tool_builtin_file_read(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioToolCustomMacro => self.test_lm_studio_tool_custom_macro(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioToolParallelExecution => self.test_lm_studio_tool_parallel_execution(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockToolRegistry => self.test_bedrock_tool_registry().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockToolBuiltinCalculator => self.test_bedrock_tool_builtin_calculator().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockToolBuiltinFileRead => self.test_bedrock_tool_builtin_file_read().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockToolCustomMacro => self.test_bedrock_tool_custom_macro().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockToolParallelExecution => self.test_bedrock_tool_parallel_execution().await.map_err(|e| format!("{}", e).into()),
            // Nova Micro tests (Core)
            TestId::NovaBasicChat => self.test_nova_basic_chat().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaMultiTurn => self.test_nova_multi_turn().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaHealthCheck => self.test_nova_health_check().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaCapabilities => self.test_nova_capabilities().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaConfiguration => self.test_nova_configuration().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaProviderRegistry => self.test_nova_provider_registry().await.map_err(|e| format!("{}", e).into()),
            // Nova Micro tool tests
            TestId::NovaToolRegistry => self.test_nova_tool_registry().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaToolBuiltinCalculator => self.test_nova_tool_builtin_calculator().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaToolBuiltinFileRead => self.test_nova_tool_builtin_file_read().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaToolCustomMacro => self.test_nova_tool_custom_macro().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaToolParallelExecution => self.test_nova_tool_parallel_execution().await.map_err(|e| format!("{}", e).into()),
            // Streaming tests (Milestone 3)
            TestId::LmStudioBasicStreaming => self.test_lm_studio_basic_streaming(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::LmStudioStreamingWithTools => self.test_lm_studio_streaming_with_tools(&test_case.model_name).await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockBasicStreaming => self.test_bedrock_basic_streaming().await.map_err(|e| format!("{}", e).into()),
            TestId::BedrockStreamingWithTools => self.test_bedrock_streaming_with_tools().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaBasicStreaming => self.test_nova_basic_streaming().await.map_err(|e| format!("{}", e).into()),
            TestId::NovaStreamingWithTools => self.test_nova_streaming_with_tools().await.map_err(|e| format!("{}", e).into()),
            // Token counting tests (Telemetry) - use provider and model from test case
            TestId::LmStudioStreamingTokenCounting => {
                let provider_str = match test_case.provider {
                    Provider::LmStudio => "lm_studio",
                    Provider::Bedrock => "bedrock",
                };
                self.test_token_counting_streaming(provider_str, &test_case.model_name).await.map_err(|e| format!("{}", e).into())
            },
            TestId::LmStudioNonStreamingTokenCounting => {
                let provider_str = match test_case.provider {
                    Provider::LmStudio => "lm_studio",
                    Provider::Bedrock => "bedrock",
                };
                self.test_token_counting_non_streaming(provider_str, &test_case.model_name).await.map_err(|e| format!("{}", e).into())
            },
            TestId::LmStudioStreamingTokenCountingWithTools => {
                let provider_str = match test_case.provider {
                    Provider::LmStudio => "lm_studio",
                    Provider::Bedrock => "bedrock",
                };
                self.test_token_counting_streaming_with_tools(provider_str, &test_case.model_name).await.map_err(|e| format!("{}", e).into())
            },
            TestId::LmStudioTokenCountingConsistency => {
                let provider_str = match test_case.provider {
                    Provider::LmStudio => "lm_studio",
                    Provider::Bedrock => "bedrock",
                };
                self.test_token_counting_consistency(provider_str, &test_case.model_name).await.map_err(|e| format!("{}", e).into())
            },
            TestId::ClaudeStreamingTokenCounting => self.test_token_counting_streaming("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::ClaudeNonStreamingTokenCounting => self.test_token_counting_non_streaming("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::ClaudeStreamingTokenCountingWithTools => self.test_token_counting_streaming_with_tools("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::ClaudeTokenCountingConsistency => self.test_token_counting_consistency("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::NovaStreamingTokenCounting => self.test_token_counting_streaming("bedrock", "us.amazon.nova-micro-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::NovaNonStreamingTokenCounting => self.test_token_counting_non_streaming("bedrock", "us.amazon.nova-micro-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::NovaStreamingTokenCountingWithTools => self.test_token_counting_streaming_with_tools("bedrock", "us.amazon.nova-micro-v1:0").await.map_err(|e| format!("{}", e).into()),
            TestId::NovaTokenCountingConsistency => self.test_token_counting_consistency("bedrock", "us.amazon.nova-micro-v1:0").await.map_err(|e| format!("{}", e).into()),
        }
    }

    
    
    
    fn print_summary(&self) {
        let total_passed = self.results.iter().filter(|r| r.passed).count();
        let total_tests = self.results.len();
        let success_rate = if total_tests == 0 { 0.0 } else { (total_passed as f64 / total_tests as f64) * 100.0 };
        
        println!("\n📊 Summary: {}/{} tests passed ({:.0}%)", total_passed, total_tests, success_rate);
        
        // Group results by provider/suite for better reporting
        let mut suite_results: HashMap<(Provider, TestSuite), Vec<&TestResult>> = HashMap::new();
        for result in &self.results {
            suite_results.entry((result.provider.clone(), result.suite.clone()))
                .or_default()
                .push(result);
        }
        
        // Print suite summaries
        for ((provider, suite), results) in &suite_results {
            let passed = results.iter().filter(|r| r.passed).count();
            let total = results.len();
            let avg_duration = results.iter().map(|r| r.duration_ms).sum::<u64>() as f64 / results.len() as f64 / 1000.0;
            
            if passed == total {
                println!("✅ {}/{}: {}/{} ({:.1}s avg)", suite.as_str(), provider.display_name(), passed, total, avg_duration);
            } else {
                println!("❌ {}/{}: {}/{} ({:.1}s avg)", suite.as_str(), provider.display_name(), passed, total, avg_duration);
                
                // Show failed tests
                for result in results.iter().filter(|r| !r.passed) {
                    println!("   • {}: {}", result.test_name, result.error.as_ref().unwrap_or(&"Unknown error".to_string()));
                }
            }
        }
        
        
        if success_rate == 100.0 {
            println!("\n🎉 Excellent - All tests passing!");
        } else if success_rate >= 75.0 {
            println!("\n✅ Good - Minor issues to address");
        } else {
            println!("\n⚠️  Issues detected - Review failures above");
        }
    }
    
    // === Test Implementation Methods ===
    
    async fn test_lm_studio_basic_chat(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Respond briefly.")
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Respond briefly.")
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Respond briefly.")
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
            
        let response = agent.execute("What is 2+2?").await?;
        
        if !response.success {
            return Err(format!("LM Studio agent execution failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Empty response from LM Studio".into());
        }
        
        // Verify response contains mathematical content
        let response_lower = response.response.to_lowercase();
        if !response_lower.contains("4") && !response_lower.contains("four") {
            return Err(format!("LM Studio response doesn't contain expected mathematical result: {}", response.response).into());
        }
        
        Ok(())
    }
    
    async fn test_lm_studio_multi_turn(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Respond briefly.")
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Respond briefly.")
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Respond briefly.")
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
            
        // First turn
        let response1 = agent.execute("My name is Alice").await?;
        
        if !response1.success {
            return Err(format!("LM Studio first turn failed: {}", response1.error.unwrap_or_default()).into());
        }
        
        if response1.response.trim().is_empty() {
            return Err("Empty first response from LM Studio".into());
        }
        
        // Second turn - test memory
        let response2 = agent.execute("What is my name?").await?;
        
        if !response2.success {
            return Err(format!("LM Studio second turn failed: {}", response2.error.unwrap_or_default()).into());
        }
        
        if response2.response.trim().is_empty() {
            return Err("Empty second response from LM Studio".into());
        }
        
        // Verify the agent remembered the name
        let response_lower = response2.response.to_lowercase();
        if !response_lower.contains("alice") {
            return Err(format!("LM Studio failed to remember name 'Alice' in conversation. Response: {}", response2.response).into());
        }
        
        Ok(())
    }
    
    async fn test_bedrock_haiku_basic_chat(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("No AWS credentials found. Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let mut agent = Agent::builder()
            .model(Bedrock::Claude35Haiku)
            .system_prompt("You are a helpful assistant. Respond briefly.")
            .build()
            .await
            .map_err(|e| format!("Failed to build Bedrock agent: {}", e))?;
            
        let response = agent.execute("What is 2+2?").await
            .map_err(|e| format!("Failed to execute Bedrock request: {}", e))?;
        
        if !response.success {
            return Err(format!("Bedrock request failed: {}", response.error.unwrap_or_else(|| "Unknown error".to_string())).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Bedrock returned empty response but no error".into());
        }
        
        Ok(())
    }
    
    async fn test_bedrock_haiku_multi_turn(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("No AWS credentials found. Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let mut agent = Agent::builder()
            .model(Bedrock::Claude35Haiku)
            .system_prompt("You are a helpful assistant. Respond briefly.")
            .build()
            .await
            .map_err(|e| format!("Failed to build Bedrock agent: {}", e))?;
            
        // First turn
        let response1 = agent.execute("My name is Bob").await
            .map_err(|e| format!("Failed to execute first Bedrock request: {}", e))?;
        
        if !response1.success {
            return Err(format!("First Bedrock request failed: {}", response1.error.unwrap_or_else(|| "Unknown error".to_string())).into());
        }
        
        if response1.response.trim().is_empty() {
            return Err("First Bedrock response was empty but no error".into());
        }
        
        // Second turn - test memory
        let response2 = agent.execute("What is my name?").await
            .map_err(|e| format!("Failed to execute second Bedrock request: {}", e))?;
        
        if !response2.success {
            return Err(format!("Second Bedrock request failed: {}", response2.error.unwrap_or_else(|| "Unknown error".to_string())).into());
        }
        
        if response2.response.trim().is_empty() {
            return Err("Second Bedrock response was empty but no error".into());
        }
        
        Ok(())
    }
    
    // === New Health Check and Configuration Tests ===
    
    async fn test_lm_studio_health_check(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::providers::LMStudioProvider;
        use stood::llm::traits::LlmProvider;
        
        let provider = LMStudioProvider::new("http://localhost:1234".to_string()).await?;
        let health = provider.health_check().await?;
        
        if !health.healthy {
            return Err(format!("LM Studio health check failed: {:?}", health.error).into());
        }
        
        if health.latency_ms.is_none() {
            return Err("Health check should include latency measurement".into());
        }
        
        Ok(())
    }
    
    async fn test_lm_studio_capabilities(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::providers::LMStudioProvider;
        use stood::llm::traits::LlmProvider;
        
        let provider = LMStudioProvider::new("http://localhost:1234".to_string()).await?;
        let capabilities = provider.capabilities();
        
        if capabilities.available_models.is_empty() {
            return Err("Provider should report available models".into());
        }
        
        // NOTE: LM Studio doesn't dynamically report max_tokens through its API
        // This is a known limitation - LM Studio uses static defaults (4096)
        // The test should validate that some max_tokens value is set, even if static
        if capabilities.max_tokens.is_none() {
            return Err("Provider should report max token limits (even if static default)".into());
        }
        
        // Verify streaming and tools support
        if !capabilities.supports_streaming {
            return Err("LM Studio should support streaming".into());
        }
        
        Ok(())
    }
    
    async fn test_lm_studio_configuration(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        
        // Test temperature configuration
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .temperature(0.1)
                    .max_tokens(50)
                    .system_prompt("Respond with exactly one word: 'test'")
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .temperature(0.1)
                    .max_tokens(50)
                    .system_prompt("Respond with exactly one word: 'test'")
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .temperature(0.1)
                    .max_tokens(50)
                    .system_prompt("Respond with exactly one word: 'test'")
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
            
        let response = agent.execute("Say the word test").await?;
        
        if !response.success {
            return Err(format!("LM Studio configuration test failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Configuration test failed - empty response".into());
        }
        
        // Verify response contains the expected word
        let response_lower = response.response.to_lowercase();
        if !response_lower.contains("test") {
            return Err(format!("LM Studio configuration test - response doesn't contain expected word 'test': {}", response.response).into());
        }
        
        // Verify the response is reasonable for max_tokens=50 setting
        // 50 tokens ≈ 150-250 characters typically, but can vary by model
        // This is a sanity check, not a strict validation since token counting varies
        if response.response.len() > 500 {
            return Err("max_tokens configuration may not be working correctly - response too long".into());
        }
        
        Ok(())
    }
    
    async fn test_bedrock_health_check(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::providers::BedrockProvider;
        use stood::llm::traits::LlmProvider;
        
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("No AWS credentials found. Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let provider = BedrockProvider::new(None).await?;
        let health = provider.health_check().await?;
        
        if !health.healthy {
            return Err(format!("Bedrock health check failed: {:?}", health.error).into());
        }
        
        Ok(())
    }
    
    async fn test_bedrock_capabilities(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::providers::BedrockProvider;
        use stood::llm::traits::LlmProvider;
        
        let provider = BedrockProvider::new(None).await?;
        let capabilities = provider.capabilities();
        
        if capabilities.available_models.is_empty() {
            return Err("Bedrock should report available models".into());
        }
        
        if !capabilities.supports_streaming {
            return Err("Bedrock should support streaming".into());
        }
        
        if !capabilities.supports_tools {
            return Err("Bedrock should support tools".into());
        }
        
        // Verify Claude models are available
        let has_claude = capabilities.available_models.iter()
            .any(|m| m.contains("claude"));
        if !has_claude {
            return Err("Bedrock should include Claude models".into());
        }
        
        Ok(())
    }
    
    async fn test_bedrock_configuration(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("No AWS credentials found. Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        // Test temperature and max_tokens configuration
        let mut agent = Agent::builder()
            .model(Bedrock::Claude35Haiku)
            .temperature(0.1)
            .max_tokens(50)
            .system_prompt("Respond with exactly one word: 'test'")
            .build()
            .await
            .map_err(|e| format!("Failed to build Bedrock agent: {}", e))?;
            
        let response = agent.execute("Say the word test").await
            .map_err(|e| format!("Failed to execute Bedrock request: {}", e))?;
        
        if !response.success {
            return Err(format!("Bedrock configuration test failed: {}", response.error.unwrap_or_else(|| "Unknown error".to_string())).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Configuration test failed - empty response".into());
        }
        
        // Verify the response is reasonable for max_tokens=50 setting
        // 50 tokens ≈ 150-250 characters typically, but can vary by model
        // This is a sanity check, not a strict validation since token counting varies
        if response.response.len() > 500 {
            return Err("max_tokens configuration may not be working correctly - response too long".into());
        }
        
        Ok(())
    }
    
    // === Provider Registry Tests ===
    
    async fn test_provider_registry_lm_studio(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::registry::{ProviderRegistry, PROVIDER_REGISTRY};
        use stood::llm::traits::ProviderType;
        
        // Test 1: Registry configuration
        ProviderRegistry::configure().await?;
        
        // Test 2: Check if LM Studio is configured
        let is_configured = PROVIDER_REGISTRY.is_configured(ProviderType::LmStudio).await;
        if !is_configured {
            return Err("LM Studio should be configured in registry".into());
        }
        
        // Test 3: Get provider instance
        let provider = PROVIDER_REGISTRY.get_provider(ProviderType::LmStudio).await?;
        if provider.provider_type() != ProviderType::LmStudio {
            return Err("Registry returned wrong provider type".into());
        }
        
        // Test 4: Verify provider works
        let health = provider.health_check().await?;
        if !health.healthy {
            return Err(format!("Provider from registry not healthy: {:?}", health.error).into());
        }
        
        Ok(())
    }
    
    async fn test_provider_registry_bedrock(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::registry::{ProviderRegistry, PROVIDER_REGISTRY};
        use stood::llm::traits::ProviderType;
        
        // Test 1: Registry configuration
        ProviderRegistry::configure().await?;
        
        // Test 2: Check if Bedrock is configured
        let is_configured = PROVIDER_REGISTRY.is_configured(ProviderType::Bedrock).await;
        if !is_configured {
            return Err("Bedrock should be configured in registry".into());
        }
        
        // Test 3: Get provider instance
        let provider = PROVIDER_REGISTRY.get_provider(ProviderType::Bedrock).await?;
        if provider.provider_type() != ProviderType::Bedrock {
            return Err("Registry returned wrong provider type".into());
        }
        
        Ok(())
    }
    
    // === Error Scenario Tests ===
    
    async fn test_lm_studio_unavailable(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::providers::LMStudioProvider;
        use stood::llm::traits::LlmProvider;
        
        // Test with invalid endpoint
        let provider = LMStudioProvider::new("http://localhost:9999".to_string()).await?;
        let health = provider.health_check().await?;
        
        if health.healthy {
            return Err("Health check should fail for unavailable endpoint".into());
        }
        
        if health.error.is_none() {
            return Err("Health check should include error message".into());
        }
        
        // Verify error message is helpful
        let error_msg = health.error.unwrap();
        if !error_msg.contains("Connection") && !error_msg.contains("refused") {
            return Err(format!("Error message not helpful: {}", error_msg).into());
        }
        
        Ok(())
    }
    
    
    async fn test_invalid_configuration(&self) -> Result<(), Box<dyn std::error::Error>> {
        // Currently the builder panics on invalid values during build
        // This is testing that the validation works correctly
        // In the future, this should return Result instead of panicking
        
        // For now, we just verify that valid configurations work
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        
        // Test valid temperature boundaries
        let _agent1 = Agent::builder()
            .model(LMStudio::Gemma3_12B)
            .temperature(0.0) // Min valid
            .build()
            .await?;
            
        let _agent2 = Agent::builder()
            .model(LMStudio::Gemma3_12B)
            .temperature(1.0) // Max valid
            .build()
            .await?;
            
        // Test valid max_tokens
        let _agent3 = Agent::builder()
            .model(LMStudio::Gemma3_12B)
            .max_tokens(1) // Min valid
            .build()
            .await?;
        
        Ok(())
    }
    
    // === Agent Builder Tests ===
    
    async fn test_agent_builder_complete(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        use stood::tools::builtin::CalculatorTool;
        
        // Test complete builder with all options
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful math assistant.")
                    .temperature(0.5)
                    .max_tokens(100)
                    .tool(Box::new(CalculatorTool))
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful math assistant.")
                    .temperature(0.5)
                    .max_tokens(100)
                    .tool(Box::new(CalculatorTool))
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful math assistant.")
                    .temperature(0.5)
                    .max_tokens(100)
                    .tool(Box::new(CalculatorTool))
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
        
        // Verify configuration was applied - use a simple request that tests the agent works
        let response = agent.execute("Hello, can you help me with math?").await?;
        
        if !response.success {
            return Err(format!("Agent builder complete test failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Agent builder complete test - empty response".into());
        }
        
        // Verify response indicates math assistance capability (agent was configured as math assistant)
        let response_lower = response.response.to_lowercase();
        if !response_lower.contains("math") && !response_lower.contains("help") && !response_lower.contains("assist") {
            return Err(format!("Agent builder test - response doesn't reflect math assistant configuration: {}", response.response).into());
        }
        
        // Note: Tool usage depends on model capability and prompt
        // We're just verifying the agent was built successfully
        
        Ok(())
    }
    
    async fn test_agent_builder_defaults(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        
        // Test minimal builder - only required field is model
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
        
        // Verify defaults work correctly
        let response = agent.execute("Say hello").await?;
        
        if !response.success {
            return Err(format!("Agent builder defaults test failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Agent builder defaults test - empty response".into());
        }
        
        // Verify response contains greeting content
        let response_lower = response.response.to_lowercase();
        if !response_lower.contains("hello") && !response_lower.contains("hi") && !response_lower.contains("greet") {
            return Err(format!("Agent builder defaults test - response doesn't contain expected greeting: {}", response.response).into());
        }
        
        Ok(())
    }

    // === Tool System Integration Tests (Milestone 2) ===

    async fn test_lm_studio_tool_registry(&self, _model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::{ToolRegistry};
        use stood::tools::builtin::CalculatorTool;

        let registry = ToolRegistry::new();
        
        // Test tool registration
        registry.register_tool(Box::new(CalculatorTool::new())).await?;
        
        // Verify registration
        if !registry.has_tool("calculator").await {
            return Err("Calculator tool not found after registration".into());
        }
        
        let tool_names = registry.tool_names().await;
        if !tool_names.contains(&"calculator".to_string()) {
            return Err("Calculator tool not in tool names list".into());
        }
        
        // Test schema generation
        let schemas = registry.get_tool_schemas().await;
        if schemas.is_empty() {
            return Err("No tool schemas generated".into());
        }
        
        // Verify schema contains calculator
        let has_calculator = schemas.iter().any(|s| s["name"] == "calculator");
        if !has_calculator {
            return Err("Calculator not found in tool schemas".into());
        }
        
        Ok(())
    }

    async fn test_lm_studio_tool_builtin_calculator(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        use stood::tools::builtin::CalculatorTool;

        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. When asked to calculate something, use the calculator tool.")
                    .tool(Box::new(CalculatorTool::new()))
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. When asked to calculate something, use the calculator tool.")
                    .tool(Box::new(CalculatorTool::new()))
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. When asked to calculate something, use the calculator tool.")
                    .tool(Box::new(CalculatorTool::new()))
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };

        // Test basic calculation request
        let response = agent.execute("What is 15 + 27? Please use the calculator tool.").await?;
        
        if !response.success {
            return Err(format!("Agent execution failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Empty response from agent with calculator tool".into());
        }
        
        // Note: We can't guarantee the model will use tools correctly, but we verify the integration works
        Ok(())
    }

    async fn test_lm_studio_tool_builtin_file_read(&self, _model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::{ToolRegistry};
        use stood::tools::builtin::FileReadTool;
        use std::fs;

        // Create a temporary test file
        let temp_dir = std::env::temp_dir();
        let temp_path = temp_dir.join("test_file_read.txt");
        let test_content = "Hello from file reading test!";
        fs::write(&temp_path, test_content)?;

        let registry = ToolRegistry::new();
        registry.register_tool(Box::new(FileReadTool::new())).await?;

        // Test file reading via tool registry
        let result = registry.execute_tool(
            "file_read",
            Some(serde_json::json!({
                "path": temp_path.to_str().unwrap()
            })),
            None
        ).await?;

        if !result.success {
            return Err(format!("File read tool failed: {}", result.error.unwrap_or_default()).into());
        }

        // Extract content from tool result (FileReadTool returns {"content": "...", "path": "..."})
        let content = result.content.get("content")
            .and_then(|v| v.as_str())
            .unwrap_or_else(|| result.content.as_str().unwrap_or(""));

        if !content.contains(test_content) {
            return Err(format!("File content mismatch. Expected '{}', got '{}'", test_content, content).into());
        }

        Ok(())
    }

    async fn test_lm_studio_tool_custom_macro(&self, _model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::ToolRegistry;

        // Define a custom tool using the macro
        use stood_macros::tool;

        #[tool(description = "Multiply two numbers together")]
        async fn multiply(
            a: f64,
            b: f64
        ) -> Result<f64, String> {
            Ok(a * b)
        }

        let registry = ToolRegistry::new();
        
        // Register the custom tool created by macro
        registry.register_tool(multiply()).await?;
        
        // Verify registration
        if !registry.has_tool("multiply").await {
            return Err("Custom multiply tool not found after registration".into());
        }
        
        // Test execution
        let result = registry.execute_tool(
            "multiply",
            Some(serde_json::json!({
                "a": 6.0,
                "b": 7.0
            })),
            None
        ).await?;
        
        if !result.success {
            return Err(format!("Custom tool execution failed: {}", result.error.unwrap_or_default()).into());
        }
        
        // Verify result
        let expected = 42.0;
        let actual = result.content.as_f64().unwrap_or(0.0);
        if (actual - expected).abs() > 0.001 {
            return Err(format!("Custom tool result mismatch. Expected {}, got {}", expected, actual).into());
        }
        
        Ok(())
    }

    async fn test_lm_studio_tool_parallel_execution(&self, _model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::ToolRegistry;
        use stood::tools::builtin::{CalculatorTool, CurrentTimeTool};
        use futures::future::join_all;

        let registry = ToolRegistry::new();
        
        // Register multiple tools
        registry.register_tool(Box::new(CalculatorTool::new())).await?;
        registry.register_tool(Box::new(CurrentTimeTool::new())).await?;
        
        // Execute tools in parallel
        let tasks = vec![
            registry.execute_tool("calculator", Some(serde_json::json!({"expression": "10 + 20"})), None),
            registry.execute_tool("calculator", Some(serde_json::json!({"expression": "5 * 8"})), None),
            registry.execute_tool("current_time", None, None),
        ];
        
        let results = join_all(tasks).await;
        
        // Verify all executions succeeded
        for (i, result) in results.into_iter().enumerate() {
            let result = result?;
            if !result.success {
                return Err(format!("Parallel execution {} failed: {}", i, result.error.unwrap_or_default()).into());
            }
        }
        
        Ok(())
    }

    // Bedrock tool tests (same implementations but with Bedrock model)

    async fn test_bedrock_tool_registry(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::{ToolRegistry};
        use stood::tools::builtin::CalculatorTool;

        let registry = ToolRegistry::new();
        
        // Test tool registration
        registry.register_tool(Box::new(CalculatorTool::new())).await?;
        
        // Verify registration
        if !registry.has_tool("calculator").await {
            return Err("Calculator tool not found after registration".into());
        }
        
        let tool_names = registry.tool_names().await;
        if !tool_names.contains(&"calculator".to_string()) {
            return Err("Calculator tool not in tool names list".into());
        }
        
        // Test schema generation
        let schemas = registry.get_tool_schemas().await;
        if schemas.is_empty() {
            return Err("No tool schemas generated".into());
        }
        
        // Verify schema contains calculator
        let has_calculator = schemas.iter().any(|s| s["name"] == "calculator");
        if !has_calculator {
            return Err("Calculator not found in tool schemas".into());
        }
        
        Ok(())
    }

    async fn test_bedrock_tool_builtin_calculator(&self) -> Result<(), Box<dyn std::error::Error>> {
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("No AWS credentials found. Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }

        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        use stood::tools::builtin::CalculatorTool;

        let mut agent = Agent::builder()
            .model(Bedrock::Claude35Haiku)
            .system_prompt("You are a helpful assistant. When asked to calculate something, use the calculator tool.")
            .tool(Box::new(CalculatorTool::new()))
            .build()
            .await
            .map_err(|e| format!("Failed to build Bedrock agent: {}", e))?;

        // Test basic calculation request
        let response = agent.execute("What is 25 + 17? Please use the calculator tool.").await
            .map_err(|e| format!("Failed to execute Bedrock request: {}", e))?;
        
        if !response.success {
            return Err(format!("Bedrock agent execution failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Empty response from Bedrock agent with calculator tool".into());
        }
        
        Ok(())
    }

    async fn test_bedrock_tool_builtin_file_read(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::{ToolRegistry};
        use stood::tools::builtin::FileReadTool;
        use std::fs;

        // Create a temporary test file
        let temp_dir = std::env::temp_dir();
        let temp_path = temp_dir.join("test_file_read_bedrock.txt");
        let test_content = "Hello from Bedrock file reading test!";
        fs::write(&temp_path, test_content)?;

        let registry = ToolRegistry::new();
        registry.register_tool(Box::new(FileReadTool::new())).await?;

        // Test file reading via tool registry
        let result = registry.execute_tool(
            "file_read",
            Some(serde_json::json!({
                "path": temp_path.to_str().unwrap()
            })),
            None
        ).await?;

        if !result.success {
            return Err(format!("File read tool failed: {}", result.error.unwrap_or_default()).into());
        }

        // Extract content from tool result (FileReadTool returns {"content": "...", "path": "..."})
        let content = result.content.get("content")
            .and_then(|v| v.as_str())
            .unwrap_or_else(|| result.content.as_str().unwrap_or(""));

        if !content.contains(test_content) {
            return Err(format!("File content mismatch. Expected '{}', got '{}'", test_content, content).into());
        }

        Ok(())
    }

    async fn test_bedrock_tool_custom_macro(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::ToolRegistry;

        // Define a custom tool using the macro
        use stood_macros::tool;

        #[tool(description = "Divide two numbers")]
        async fn divide(
            a: f64,
            b: f64
        ) -> Result<f64, String> {
            if b == 0.0 {
                return Err("Division by zero".to_string());
            }
            Ok(a / b)
        }

        let registry = ToolRegistry::new();
        
        // Register the custom tool created by macro
        registry.register_tool(divide()).await?;
        
        // Verify registration
        if !registry.has_tool("divide").await {
            return Err("Custom divide tool not found after registration".into());
        }
        
        // Test execution
        let result = registry.execute_tool(
            "divide",
            Some(serde_json::json!({
                "a": 84.0,
                "b": 2.0
            })),
            None
        ).await?;
        
        if !result.success {
            return Err(format!("Custom tool execution failed: {}", result.error.unwrap_or_default()).into());
        }
        
        // Verify result
        let expected = 42.0;
        let actual = result.content.as_f64().unwrap_or(0.0);
        if (actual - expected).abs() > 0.001 {
            return Err(format!("Custom tool result mismatch. Expected {}, got {}", expected, actual).into());
        }
        
        Ok(())
    }

    async fn test_bedrock_tool_parallel_execution(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::ToolRegistry;
        use stood::tools::builtin::{CalculatorTool, CurrentTimeTool};
        use futures::future::join_all;

        let registry = ToolRegistry::new();
        
        // Register multiple tools
        registry.register_tool(Box::new(CalculatorTool::new())).await?;
        registry.register_tool(Box::new(CurrentTimeTool::new())).await?;
        
        // Execute tools in parallel
        let tasks = vec![
            registry.execute_tool("calculator", Some(serde_json::json!({"expression": "15 + 25"})), None),
            registry.execute_tool("calculator", Some(serde_json::json!({"expression": "8 * 7"})), None),
            registry.execute_tool("current_time", None, None),
        ];
        
        let results = join_all(tasks).await;
        
        // Verify all executions succeeded
        for (i, result) in results.into_iter().enumerate() {
            let result = result?;
            if !result.success {
                return Err(format!("Parallel execution {} failed: {}", i, result.error.unwrap_or_default()).into());
            }
        }
        
        Ok(())
    }

    // Nova Micro test implementations (using NovaMicro model)
    
    async fn test_nova_basic_chat(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("Nova Micro test requires AWS credentials: Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let mut agent = Agent::builder()
            .model(Bedrock::NovaMicro)
            .system_prompt("You are a helpful assistant. Respond briefly.")
            .build()
            .await?;
            
        let response = agent.execute("What is 2+2?").await?;
        
        if !response.success {
            return Err(format!("Nova Micro agent execution failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Empty response from Nova Micro".into());
        }
        
        // Verify response contains mathematical content (basic sanity check)
        let response_lower = response.response.to_lowercase();
        if !response_lower.contains("4") && !response_lower.contains("four") {
            return Err(format!("Nova Micro response doesn't contain expected mathematical result: {}", response.response).into());
        }
        
        Ok(())
    }
    
    async fn test_nova_multi_turn(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("Nova Micro test requires AWS credentials: Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let mut agent = Agent::builder()
            .model(Bedrock::NovaMicro)
            .system_prompt("You are a helpful assistant. Respond briefly.")
            .build()
            .await?;
            
        // First turn
        let response1 = agent.execute("My name is Alice").await?;
        
        if !response1.success {
            return Err(format!("Nova Micro first turn failed: {}", response1.error.unwrap_or_default()).into());
        }
        
        if response1.response.trim().is_empty() {
            return Err("Empty first response from Nova Micro".into());
        }
        
        // Second turn - test memory
        let response2 = agent.execute("What is my name?").await?;
        
        if !response2.success {
            return Err(format!("Nova Micro second turn failed: {}", response2.error.unwrap_or_default()).into());
        }
        
        if response2.response.trim().is_empty() {
            return Err("Empty second response from Nova Micro".into());
        }
        
        // Verify the agent remembered the name (basic conversation memory test)
        let response_lower = response2.response.to_lowercase();
        if !response_lower.contains("alice") {
            return Err(format!("Nova Micro failed to remember name 'Alice' in conversation. Response: {}", response2.response).into());
        }
        
        Ok(())
    }
    
    async fn test_nova_health_check(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::traits::ProviderType;
        use stood::llm::registry::PROVIDER_REGISTRY;
        
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("Nova Micro test requires AWS credentials: Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let provider = PROVIDER_REGISTRY.get_provider(ProviderType::Bedrock).await?;
        let health = provider.health_check().await?;
        
        if !health.healthy {
            return Err(format!("Nova Micro health check failed: {:?}", health.error).into());
        }
        
        Ok(())
    }
    
    async fn test_nova_capabilities(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::models::Bedrock;
        use stood::llm::traits::LlmModel;
        
        let model = Bedrock::NovaMicro;
        let capabilities = model.capabilities();
        
        if !capabilities.supports_tools {
            return Err("Nova Micro should support tools".into());
        }
        
        if !capabilities.supports_streaming {
            return Err("Nova Micro should support streaming".into());
        }
        
        Ok(())
    }
    
    async fn test_nova_configuration(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::models::Bedrock;
        use stood::llm::traits::LlmModel;
        
        let model = Bedrock::NovaMicro;
        
        if model.model_id().is_empty() {
            return Err("Nova Micro model ID should not be empty".into());
        }
        
        if model.context_window() == 0 {
            return Err("Nova Micro context window should be > 0".into());
        }
        
        Ok(())
    }
    
    async fn test_nova_provider_registry(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::llm::traits::ProviderType;
        use stood::llm::registry::PROVIDER_REGISTRY;
        
        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("Nova Micro test requires AWS credentials: Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }
        
        let provider = PROVIDER_REGISTRY.get_provider(ProviderType::Bedrock).await?;
        
        if provider.supported_models().is_empty() {
            return Err("Nova Micro provider should support models".into());
        }
        
        Ok(())
    }
    
    
    // Nova Micro tool tests
    
    async fn test_nova_tool_registry(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::{ToolRegistry};
        use stood::tools::builtin::CalculatorTool;

        let registry = ToolRegistry::new();
        registry.register_tool(Box::new(CalculatorTool::new())).await?;

        let tool_names = registry.tool_names().await;
        if !tool_names.contains(&"calculator".to_string()) {
            return Err("Calculator tool should be registered".into());
        }

        Ok(())
    }

    async fn test_nova_tool_builtin_calculator(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        use stood::tools::builtin::CalculatorTool;

        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("Nova Micro test requires AWS credentials: Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }

        let mut agent = Agent::builder()
            .model(Bedrock::NovaMicro)
            .system_prompt("You are a helpful assistant. When asked to calculate something, use the calculator tool.")
            .tool(Box::new(CalculatorTool::new()))
            .build()
            .await
            .map_err(|e| format!("Failed to build Nova Micro agent: {}", e))?;

        // Test basic calculation request
        let response = agent.execute("What is 25 + 17? Please use the calculator tool.").await
            .map_err(|e| format!("Failed to execute Nova Micro request: {}", e))?;
        
        if !response.success {
            return Err(format!("Nova Micro agent execution failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Empty response from Nova Micro agent with calculator tool".into());
        }
        
        Ok(())
    }

    async fn test_nova_tool_builtin_file_read(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        use stood::tools::builtin::FileReadTool;
        use std::fs;

        // Check if AWS credentials are available
        if std::env::var("AWS_ACCESS_KEY_ID").is_err() && std::env::var("AWS_PROFILE").is_err() {
            return Err("Nova Micro test requires AWS credentials: Set AWS_ACCESS_KEY_ID or AWS_PROFILE".into());
        }

        // Create a temporary test file
        let temp_dir = std::env::temp_dir();
        let temp_path = temp_dir.join("test_file_read_nova_streaming.txt");
        let test_content = "Hello from Nova Micro streaming file reading test!";
        fs::write(&temp_path, test_content)?;

        // Test Nova file reading via Agent with streaming (this will test tool streaming)
        let mut agent = Agent::builder()
            .model(Bedrock::NovaMicro)
            .system_prompt("You are a helpful assistant. When asked to read a file, use the file_read tool and then summarize the content you found.")
            .tool(Box::new(FileReadTool::new()))
            .build()
            .await
            .map_err(|e| format!("Failed to build Nova Micro agent: {}", e))?;

        // Request file reading via streaming - this should trigger Nova tool streaming
        let response = agent.execute(&format!(
            "Please read the file at '{}' and tell me what it contains.", 
            temp_path.to_str().unwrap()
        )).await
            .map_err(|e| format!("Failed to execute Nova Micro streaming request: {}", e))?;
        
        if !response.success {
            return Err(format!("Nova Micro streaming agent execution failed: {}", response.error.unwrap_or_default()).into());
        }
        
        if response.response.trim().is_empty() {
            return Err("Empty response from Nova Micro streaming agent with file read tool".into());
        }
        
        // Verify the response mentions the file content (the agent should have used the tool)
        if !response.used_tools {
            return Err("Nova Micro agent should have used the file_read tool".into());
        }
        
        // Verify the response actually contains content from the file
        let response_lower = response.response.to_lowercase();
        if !response_lower.contains("hello from nova micro") && !response_lower.contains("streaming file reading test") {
            return Err(format!("Nova Micro response doesn't contain expected file content. Response: {}", response.response).into());
        }

        Ok(())
    }

    async fn test_nova_tool_custom_macro(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::ToolRegistry;

        // Define a custom tool using the macro
        use stood_macros::tool;

        #[tool(description = "Divide two numbers")]
        async fn divide_nova(
            a: f64,
            b: f64
        ) -> Result<f64, String> {
            if b == 0.0 {
                return Err("Division by zero".to_string());
            }
            Ok(a / b)
        }

        let registry = ToolRegistry::new();
        registry.register_tool(divide_nova()).await?;

        // Test the custom tool
        let result = registry.execute_tool(
            "divide_nova",
            Some(serde_json::json!({
                "a": 10.0,
                "b": 2.0
            })),
            None
        ).await?;

        if !result.success {
            return Err(format!("Custom tool execution failed: {}", result.error.unwrap_or_default()).into());
        }

        // Verify result
        if let Some(result_value) = result.content.as_f64() {
            if (result_value - 5.0).abs() > 0.001 {
                return Err(format!("Expected result 5.0, got {}", result_value).into());
            }
        } else {
            return Err("Tool result should be a number".into());
        }

        Ok(())
    }

    async fn test_nova_tool_parallel_execution(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::tools::ToolRegistry;
        use stood::tools::builtin::{CalculatorTool, CurrentTimeTool};
        use futures::future::join_all;

        let registry = ToolRegistry::new();
        
        // Register multiple tools
        registry.register_tool(Box::new(CalculatorTool::new())).await?;
        registry.register_tool(Box::new(CurrentTimeTool::new())).await?;
        
        // Execute tools in parallel
        let tasks = vec![
            registry.execute_tool("calculator", Some(serde_json::json!({"expression": "15 + 25"})), None),
            registry.execute_tool("calculator", Some(serde_json::json!({"expression": "8 * 7"})), None),
            registry.execute_tool("current_time", None, None),
        ];
        
        let results = join_all(tasks).await;
        
        // Verify all executions succeeded
        for (i, result) in results.into_iter().enumerate() {
            let result = result?;
            if !result.success {
                return Err(format!("Nova Micro parallel execution {} failed: {}", i, result.error.unwrap_or_default()).into());
            }
        }
        
        Ok(())
    }

    // =============================================================================
    // MILESTONE 3: Streaming Tests
    // =============================================================================

    async fn test_lm_studio_basic_streaming(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Keep responses brief.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Keep responses brief.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Keep responses brief.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
            
        // Execute request with streaming enabled
        let result = agent.execute("Count from 1 to 5, one number per sentence.").await?;
        
        // Verify streaming worked correctly
        if !result.success {
            return Err(format!("LM Studio streaming test failed: {}", result.error.unwrap_or_default()).into());
        }
        
        if result.response.trim().is_empty() {
            return Err("Empty response from LM Studio streaming".into());
        }
        
        // Verify response contains numbers (basic content check)
        let contains_numbers = ["1", "2", "3", "4", "5"].iter()
            .any(|num| result.response.contains(num));
        
        if !contains_numbers {
            // Not a hard failure, different models may respond differently
            // Silently continue - the important thing is that streaming worked
        }
        
        Ok(())
    }

    async fn test_lm_studio_streaming_with_tools(&self, model_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::LMStudio;
        use stood::tools::builtin::CalculatorTool;
        
        let mut agent = match model_name {
            "google/gemma-3-27b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            "google/gemma-3-12b" => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            "tessa-rust-t1-7b" => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported LM Studio model: {}", model_name).into()),
        };
            
        // Execute request with streaming and tools enabled
        let result = agent.execute("Calculate 17 * 29 using the calculator tool.").await?;
        
        // Verify streaming with tools worked correctly
        if !result.success {
            return Err(format!("LM Studio streaming with tools test failed: {}", result.error.unwrap_or_default()).into());
        }
        
        if result.response.trim().is_empty() {
            return Err("Empty response from LM Studio streaming with tools".into());
        }
        
        if !result.used_tools || result.tools_called.is_empty() {
            return Err("No tools were used in LM Studio streaming response".into());
        }
        
        // Verify calculator tool was used
        let calculator_used = result.tools_called.iter()
            .any(|tool_name| tool_name.contains("calculator") || tool_name.contains("calc"));
        
        if !calculator_used {
            return Err(format!("Calculator tool not used in LM Studio. Tools used: {:?}", result.tools_called).into());
        }
        
        // Verify result contains the correct answer (17 * 29 = 493)
        if !result.response.contains("493") {
            return Err(format!("Expected '493' in LM Studio response: {}", result.response).into());
        }
        
        Ok(())
    }

    async fn test_bedrock_basic_streaming(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        let mut agent = Agent::builder()
            .model(Bedrock::Claude35Haiku)
            .system_prompt("You are a helpful assistant. Keep responses brief.")
            .with_streaming(true)
            .build()
            .await?;
            
        // Execute request with streaming enabled
        let result = agent.execute("Count from 1 to 5, one number per sentence.").await?;
        
        // Verify streaming worked correctly
        if !result.success {
            return Err(format!("Bedrock streaming test failed: {}", result.error.unwrap_or_default()).into());
        }
        
        if result.response.trim().is_empty() {
            return Err("Empty response from Bedrock streaming".into());
        }
        
        // Verify response contains numbers (basic content check)
        let contains_numbers = ["1", "2", "3", "4", "5"].iter()
            .any(|num| result.response.contains(num));
        
        if !contains_numbers {
            // Not a hard failure, different models may respond differently
            // Silently continue - the important thing is that streaming worked
        }
        
        Ok(())
    }

    async fn test_bedrock_streaming_with_tools(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        use stood::tools::builtin::CalculatorTool;
        
        let mut agent = Agent::builder()
            .model(Bedrock::Claude35Haiku)
            .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
            .tool(Box::new(CalculatorTool::new()))
            .with_streaming(true)
            .build()
            .await?;
            
        // Execute request with streaming and tools enabled
        let result = agent.execute("Calculate 17 * 29 using the calculator tool.").await?;
        
        // Verify streaming with tools worked correctly
        if !result.success {
            return Err(format!("Bedrock streaming with tools test failed: {}", result.error.unwrap_or_default()).into());
        }
        
        if result.response.trim().is_empty() {
            return Err("Empty response from Bedrock streaming with tools".into());
        }
        
        if !result.used_tools || result.tools_called.is_empty() {
            return Err("No tools were used in Bedrock streaming response".into());
        }
        
        // Verify calculator tool was used
        let calculator_used = result.tools_called.iter()
            .any(|tool_name| tool_name.contains("calculator") || tool_name.contains("calc"));
        
        if !calculator_used {
            return Err(format!("Calculator tool not used in Bedrock. Tools used: {:?}", result.tools_called).into());
        }
        
        // Verify result contains the correct answer (17 * 29 = 493)
        if !result.response.contains("493") {
            return Err(format!("Expected '493' in Bedrock response: {}", result.response).into());
        }
        
        Ok(())
    }

    async fn test_nova_basic_streaming(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        
        let mut agent = Agent::builder()
            .model(Bedrock::NovaMicro)
            .system_prompt("You are a helpful assistant. Keep responses brief.")
            .with_streaming(true)
            .build()
            .await?;
            
        // Execute request with streaming enabled
        let result = agent.execute("Count from 1 to 5, one number per sentence.").await?;
        
        // Verify streaming worked correctly
        if !result.success {
            return Err(format!("Nova streaming test failed: {}", result.error.unwrap_or_default()).into());
        }
        
        if result.response.trim().is_empty() {
            return Err("Empty response from Nova streaming".into());
        }
        
        // Verify response contains numbers (basic content check)
        let contains_numbers = ["1", "2", "3", "4", "5"].iter()
            .any(|num| result.response.contains(num));
        
        if !contains_numbers {
            // Not a hard failure, different models may respond differently
            // Silently continue - the important thing is that streaming worked
        }
        
        Ok(())
    }

    async fn test_nova_streaming_with_tools(&self) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::Bedrock;
        use stood::tools::builtin::CalculatorTool;
        
        let mut agent = Agent::builder()
            .model(Bedrock::NovaMicro)
            .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
            .tool(Box::new(CalculatorTool::new()))
            .with_streaming(true)
            .build()
            .await?;
            
        // Execute request with streaming and tools enabled
        let result = agent.execute("Calculate 17 * 29 using the calculator tool.").await?;
        
        // Verify streaming with tools worked correctly
        if !result.success {
            return Err(format!("Nova streaming with tools test failed: {}", result.error.unwrap_or_default()).into());
        }
        
        if result.response.trim().is_empty() {
            return Err("Empty response from Nova streaming with tools".into());
        }
        
        if !result.used_tools || result.tools_called.is_empty() {
            return Err("No tools were used in Nova streaming response".into());
        }
        
        // Verify calculator tool was used
        let calculator_used = result.tools_called.iter()
            .any(|tool_name| tool_name.contains("calculator") || tool_name.contains("calc"));
        
        if !calculator_used {
            return Err(format!("Calculator tool not used in Nova. Tools used: {:?}", result.tools_called).into());
        }
        
        // Verify result contains the correct answer (17 * 29 = 493)
        if !result.response.contains("493") {
            return Err(format!("Expected '493' in Nova response: {}", result.response).into());
        }
        
        Ok(())
    }

    // Token counting test implementations
    async fn test_token_counting_streaming(&self, provider: &str, model_id: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::{Bedrock, LMStudio};
        
        let mut agent = match (provider, model_id) {
            ("lm_studio", "google/gemma-3-27b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("lm_studio", "google/gemma-3-12b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("lm_studio", "tessa-rust-t1-7b") => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0") => {
                Agent::builder()
                    .model(Bedrock::Claude35Haiku)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("bedrock", "us.amazon.nova-micro-v1:0") => {
                Agent::builder()
                    .model(Bedrock::NovaMicro)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported provider/model: {}/{}", provider, model_id).into()),
        };
        
        let result = agent.execute("Explain what 2+2 equals in exactly one sentence.").await?;
        
        // Verify token information is available
        let tokens = result.execution.tokens
            .ok_or("No token usage information available")?;
        
        // Verify token counts are non-zero
        if tokens.total_tokens == 0 {
            return Err("Total tokens is zero - token counting failed".into());
        }
        
        if tokens.input_tokens == 0 {
            return Err("Input tokens is zero - input token counting failed".into());
        }
        
        if tokens.output_tokens == 0 {
            return Err("Output tokens is zero - output token counting failed".into());
        }
        
        // Verify token arithmetic
        if tokens.total_tokens != tokens.input_tokens + tokens.output_tokens {
            return Err(format!(
                "Token arithmetic incorrect: {} != {} + {}",
                tokens.total_tokens, tokens.input_tokens, tokens.output_tokens
            ).into());
        }
        
        // Verify streaming was used
        if !result.execution.performance.was_streamed {
            return Err("Response was not streamed despite streaming being enabled".into());
        }
        
        Ok(())
    }

    async fn test_token_counting_non_streaming(&self, provider: &str, model_id: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::{Bedrock, LMStudio};
        
        let mut agent = match (provider, model_id) {
            ("lm_studio", "google/gemma-3-27b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("lm_studio", "google/gemma-3-12b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("lm_studio", "tessa-rust-t1-7b") => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0") => {
                Agent::builder()
                    .model(Bedrock::Claude35Haiku)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("bedrock", "us.amazon.nova-micro-v1:0") => {
                Agent::builder()
                    .model(Bedrock::NovaMicro)
                    .system_prompt("You are a helpful assistant. Respond concisely.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported provider/model: {}/{}", provider, model_id).into()),
        };
        
        let result = agent.execute("What is the capital of France? Answer in one word.").await?;
        
        // Verify token information is available
        let tokens = result.execution.tokens
            .ok_or("No token usage information available")?;
        
        // Verify token counts are non-zero
        if tokens.total_tokens == 0 {
            return Err("Total tokens is zero - token counting failed".into());
        }
        
        if tokens.input_tokens == 0 {
            return Err("Input tokens is zero - input token counting failed".into());
        }
        
        if tokens.output_tokens == 0 {
            return Err("Output tokens is zero - output token counting failed".into());
        }
        
        // Verify token arithmetic
        if tokens.total_tokens != tokens.input_tokens + tokens.output_tokens {
            return Err(format!(
                "Token arithmetic incorrect: {} != {} + {}",
                tokens.total_tokens, tokens.input_tokens, tokens.output_tokens
            ).into());
        }
        
        // Verify streaming was NOT used
        if result.execution.performance.was_streamed {
            return Err("Response was streamed despite streaming being disabled".into());
        }
        
        Ok(())
    }

    async fn test_token_counting_streaming_with_tools(&self, provider: &str, model_id: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::{Bedrock, LMStudio};
        use stood::tools::builtin::CalculatorTool;
        
        let mut agent = match (provider, model_id) {
            ("lm_studio", "google/gemma-3-27b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("lm_studio", "google/gemma-3-12b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("lm_studio", "tessa-rust-t1-7b") => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0") => {
                Agent::builder()
                    .model(Bedrock::Claude35Haiku)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("bedrock", "us.amazon.nova-micro-v1:0") => {
                Agent::builder()
                    .model(Bedrock::NovaMicro)
                    .system_prompt("You are a helpful assistant with access to tools. Use the calculator tool for math problems.")
                    .tool(Box::new(CalculatorTool::new()))
                    .with_streaming(true)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported provider/model: {}/{}", provider, model_id).into()),
        };
        
        let result = agent.execute("Calculate 15 * 23 using the calculator tool.").await?;
        
        // Verify tools were used
        if !result.used_tools || result.tools_called.is_empty() {
            return Err("No tools were used in the response".into());
        }
        
        // Verify token information is available
        let tokens = result.execution.tokens
            .ok_or("No token usage information available")?;
        
        // Verify token counts are non-zero
        if tokens.total_tokens == 0 {
            return Err("Total tokens is zero - token counting failed".into());
        }
        
        if tokens.input_tokens == 0 {
            return Err("Input tokens is zero - input token counting failed".into());
        }
        
        if tokens.output_tokens == 0 {
            return Err("Output tokens is zero - output token counting failed".into());
        }
        
        // Verify token arithmetic
        if tokens.total_tokens != tokens.input_tokens + tokens.output_tokens {
            return Err(format!(
                "Token arithmetic incorrect: {} != {} + {}",
                tokens.total_tokens, tokens.input_tokens, tokens.output_tokens
            ).into());
        }
        
        // Verify streaming was used
        if !result.execution.performance.was_streamed {
            return Err("Response was not streamed despite streaming being enabled".into());
        }
        
        Ok(())
    }

    async fn test_token_counting_consistency(&self, provider: &str, model_id: &str) -> Result<(), Box<dyn std::error::Error>> {
        use stood::agent::Agent;
        use stood::llm::models::{Bedrock, LMStudio};
        
        let test_prompt = "Count from 1 to 3, with each number on a separate line.";
        
        // Create streaming agent
        let mut streaming_agent = match (provider, model_id) {
            ("lm_studio", "google/gemma-3-27b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("lm_studio", "google/gemma-3-12b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("lm_studio", "tessa-rust-t1-7b") => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0") => {
                Agent::builder()
                    .model(Bedrock::Claude35Haiku)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            ("bedrock", "us.amazon.nova-micro-v1:0") => {
                Agent::builder()
                    .model(Bedrock::NovaMicro)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(true)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported provider/model: {}/{}", provider, model_id).into()),
        };
        
        // Create non-streaming agent
        let mut non_streaming_agent = match (provider, model_id) {
            ("lm_studio", "google/gemma-3-27b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_27B)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("lm_studio", "google/gemma-3-12b") => {
                Agent::builder()
                    .model(LMStudio::Gemma3_12B)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("lm_studio", "tessa-rust-t1-7b") => {
                Agent::builder()
                    .model(LMStudio::TessaRust7B)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("bedrock", "us.anthropic.claude-3-5-haiku-20241022-v1:0") => {
                Agent::builder()
                    .model(Bedrock::Claude35Haiku)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            ("bedrock", "us.amazon.nova-micro-v1:0") => {
                Agent::builder()
                    .model(Bedrock::NovaMicro)
                    .system_prompt("You are a helpful assistant. Follow instructions exactly.")
                    .with_streaming(false)
                    .build()
                    .await?
            }
            _ => return Err(format!("Unsupported provider/model: {}/{}", provider, model_id).into()),
        };
        
        // Execute the same prompt with both modes
        let streaming_result = streaming_agent.execute(test_prompt).await?;
        let non_streaming_result = non_streaming_agent.execute(test_prompt).await?;
        
        // Verify both responses have token information
        let streaming_tokens = streaming_result.execution.tokens
            .ok_or("No token usage information available for streaming response")?;
        
        let non_streaming_tokens = non_streaming_result.execution.tokens
            .ok_or("No token usage information available for non-streaming response")?;
        
        // Verify streaming flag is correct
        if !streaming_result.execution.performance.was_streamed {
            return Err("Streaming response was not marked as streamed".into());
        }
        
        if non_streaming_result.execution.performance.was_streamed {
            return Err("Non-streaming response was incorrectly marked as streamed".into());
        }
        
        // For consistency test, we just verify both have valid token counts
        // Some variance between modes is expected, especially for estimation-based providers
        if streaming_tokens.total_tokens == 0 || non_streaming_tokens.total_tokens == 0 {
            return Err("Token counting failed in one or both modes".into());
        }
        
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Disable telemetry for clean verification output unless explicitly enabled
    if std::env::var("STOOD_TELEMETRY").is_err() {
        std::env::set_var("OTEL_SDK_DISABLED", "true");
        std::env::set_var("RUST_LOG", "error");
        std::env::set_var("OTEL_EXPORTER_OTLP_ENDPOINT", ""); // Disable OTLP export
        std::env::set_var("TRACING_DISABLED", "true");
    }
    let matches = Command::new("verify")
        .about("Simplified verification runner for Stood LLM Client")
        .arg(Arg::new("suite")
            .help("Test suite to run (core, tools, streaming, token_counting, advanced)")
            .value_parser(["core", "tools", "streaming", "token_counting", "advanced"])
            .index(1))
        .arg(Arg::new("provider")
            .long("provider")
            .short('p') 
            .help("Provider to test (lm_studio, bedrock, haiku)")
            .value_parser(["lm_studio", "bedrock", "haiku"]))
        .arg(Arg::new("test")
            .long("test")
            .short('t')
            .help("Specific test name to run (e.g., builtin_file_read, tool_registry)")
            .action(clap::ArgAction::Append))
        .arg(Arg::new("model")
            .long("model")
            .short('m')
            .help("Model name to filter tests (e.g., claude-3-5-haiku, google/gemma-3-27b)")
            .action(clap::ArgAction::Append))
        .arg(Arg::new("debug")
            .long("debug")
            .short('d')
            .help("Enable debug output")
            .action(clap::ArgAction::SetTrue))
        .get_matches();
    
    // Build filters from command line arguments
    let mut filters = TestFilters::new();
    
    // Set suite filter
    if let Some(suite_str) = matches.get_one::<String>("suite") {
        filters.suites = Some(vec![TestSuite::from_str(suite_str).unwrap()]);
    }
    
    // Set provider filter
    if let Some(provider_str) = matches.get_one::<String>("provider") {
        filters.providers = Some(vec![Provider::from_str(provider_str).unwrap()]);
    }
    
    // Set test name filter
    if let Some(test_names) = matches.get_many::<String>("test") {
        filters.test_names = Some(test_names.cloned().collect());
    }
    
    // Set model filter
    if let Some(models) = matches.get_many::<String>("model") {
        filters.models = Some(models.cloned().collect());
    }
    
    // Set debug flag
    filters.debug = matches.get_flag("debug");
    
    // Run verification with filters
    let mut runner = VerificationRunner::new();
    runner.run(filters).await?;
    
    Ok(())
}