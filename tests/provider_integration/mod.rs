//! LLM Client Verification Test Suite
//!
//! This module provides comprehensive verification testing for all LLM providers
//! and models supported by the Stood library.

pub mod shared;
pub mod providers;
pub mod integration;

// Re-export key types for convenience
pub use shared::*;

use crate::llm::traits::ProviderType;
use std::collections::HashMap;

/// Main verification runner for all providers
pub struct VerificationRunner;

impl VerificationRunner {
    /// Run verification tests for all available providers
    pub async fn run_all() -> HashMap<ProviderType, Vec<VerificationResult>> {
        println!("üöÄ Starting Comprehensive LLM Client Verification");
        println!("==================================================");
        
        // Detect available providers
        let env = shared::config::TestEnvironment::detect().await;
        env.print_summary();
        
        if env.available_providers.is_empty() {
            println!("‚ùå No providers available for testing");
            println!("   Please ensure at least one provider is configured:");
            println!("   - LM Studio: Start LM Studio with API enabled");
            println!("   - AWS Bedrock: Set AWS credentials");
            println!("   - Anthropic: Set ANTHROPIC_API_KEY");
            println!("   - OpenAI: Set OPENAI_API_KEY");
            return HashMap::new();
        }
        
        let mut results = HashMap::new();
        
        // Run tests for each available provider
        for provider in &env.available_providers {
            println!("\nüìã Testing Provider: {:?}", provider);
            println!("{}", "=".repeat(40));
            
            let provider_results = match provider {
                ProviderType::LmStudio => {
                    providers::lm_studio::run_all_tests().await
                }
                ProviderType::Bedrock => {
                    // TODO: Implement bedrock verification
                    println!("‚è≠Ô∏è  Bedrock verification not yet implemented");
                    Vec::new()
                }
                ProviderType::Anthropic => {
                    // TODO: Implement anthropic verification
                    println!("‚è≠Ô∏è  Anthropic verification not yet implemented");
                    Vec::new()
                }
                _ => {
                    println!("‚è≠Ô∏è  Verification not implemented for {:?}", provider);
                    Vec::new()
                }
            };
            
            if !provider_results.is_empty() {
                results.insert(*provider, provider_results);
            }
        }
        
        // Generate cross-provider comparison report
        if results.len() > 1 {
            println!("\nüìä Cross-Provider Comparison");
            println!("============================");
            let comparison = shared::generate_comparison_report(&results);
            println!("{}", comparison);
        }
        
        Self::print_overall_summary(&results);
        results
    }
    
    /// Run verification tests for a specific provider
    pub async fn run_provider(provider: ProviderType) -> Vec<VerificationResult> {
        println!("üéØ Running Verification for Provider: {:?}", provider);
        println!("{}", "=".repeat(50));
        
        // Check if provider is available
        let env = shared::config::TestEnvironment::detect().await;
        if !env.has_provider(provider) {
            println!("‚ùå Provider {:?} is not available", provider);
            return Vec::new();
        }
        
        match provider {
            ProviderType::LmStudio => {
                providers::lm_studio::run_all_tests().await
            }
            ProviderType::Bedrock => {
                println!("‚è≠Ô∏è  Bedrock verification not yet implemented");
                Vec::new()
            }
            ProviderType::Anthropic => {
                println!("‚è≠Ô∏è  Anthropic verification not yet implemented");
                Vec::new()
            }
            _ => {
                println!("‚è≠Ô∏è  Verification not implemented for {:?}", provider);
                Vec::new()
            }
        }
    }
    
    /// Run specific milestone tests across all providers
    pub async fn run_milestone(milestone: u8) -> HashMap<ProviderType, Vec<VerificationResult>> {
        println!("üéØ Running MILESTONE {} across all providers", milestone);
        println!("{}={}", "=".repeat(milestone_name(milestone).len() + 20), "=".repeat(20));
        
        let env = shared::config::TestEnvironment::detect().await;
        let mut results = HashMap::new();
        
        for provider in &env.available_providers {
            println!("\nüìã MILESTONE {} - Provider: {:?}", milestone, provider);
            
            let provider_results = match provider {
                ProviderType::LmStudio => {
                    providers::lm_studio::run_milestone_tests(milestone).await
                }
                _ => {
                    println!("‚è≠Ô∏è  Milestone tests not implemented for {:?}", provider);
                    Vec::new()
                }
            };
            
            if !provider_results.is_empty() {
                results.insert(*provider, provider_results);
            }
        }
        
        results
    }
    
    /// Run performance benchmarks for all providers
    pub async fn run_performance_tests() -> HashMap<ProviderType, Vec<VerificationResult>> {
        println!("üèÉ‚Äç‚ôÇÔ∏è Running Performance Tests for All Providers");
        println!("================================================");
        
        let env = shared::config::TestEnvironment::detect().await;
        let mut results = HashMap::new();
        
        for provider in &env.available_providers {
            println!("\n‚ö° Performance Testing - Provider: {:?}", provider);
            
            let provider_results = match provider {
                ProviderType::LmStudio => {
                    providers::lm_studio::run_performance_tests().await
                }
                _ => {
                    println!("‚è≠Ô∏è  Performance tests not implemented for {:?}", provider);
                    Vec::new()
                }
            };
            
            if !provider_results.is_empty() {
                results.insert(*provider, provider_results);
            }
        }
        
        Self::print_performance_summary(&results);
        results
    }
    
    /// Print overall summary across all providers
    fn print_overall_summary(results: &HashMap<ProviderType, Vec<VerificationResult>>) {
        println!("\nüéØ OVERALL VERIFICATION SUMMARY");
        println!("===============================");
        
        let mut total_tests = 0;
        let mut total_passed = 0;
        let mut total_failed = 0;
        
        for (provider, provider_results) in results {
            let passed = provider_results.iter().filter(|r| r.success).count();
            let failed = provider_results.len() - passed;
            
            total_tests += provider_results.len();
            total_passed += passed;
            total_failed += failed;
            
            let success_rate = (passed as f64 / provider_results.len() as f64) * 100.0;
            println!("  {:?}: {}/{} tests passed ({:.1}%)", 
                provider, passed, provider_results.len(), success_rate);
        }
        
        if total_tests > 0 {
            let overall_success_rate = (total_passed as f64 / total_tests as f64) * 100.0;
            println!("\nüìä OVERALL RESULTS:");
            println!("   Total Tests: {}", total_tests);
            println!("   Passed: {} ‚úÖ", total_passed);
            println!("   Failed: {} ‚ùå", total_failed);
            println!("   Success Rate: {:.1}%", overall_success_rate);
            
            if overall_success_rate >= 90.0 {
                println!("\nüéâ LLM Client Verification: EXCELLENT - Ready for production!");
            } else if overall_success_rate >= 75.0 {
                println!("\n‚úÖ LLM Client Verification: GOOD - Minor issues to address");
            } else if overall_success_rate >= 50.0 {
                println!("\n‚ö†Ô∏è  LLM Client Verification: NEEDS IMPROVEMENT - Several issues found");
            } else {
                println!("\n‚ùå LLM Client Verification: POOR - Major issues need attention");
            }
        }
    }
    
    /// Print performance summary
    fn print_performance_summary(results: &HashMap<ProviderType, Vec<VerificationResult>>) {
        println!("\n‚ö° PERFORMANCE SUMMARY");
        println!("=====================");
        
        for (provider, provider_results) in results {
            let successful_results: Vec<_> = provider_results.iter().filter(|r| r.success).collect();
            
            if successful_results.is_empty() {
                println!("  {:?}: No successful tests for performance analysis", provider);
                continue;
            }
            
            let avg_duration = successful_results.iter()
                .map(|r| r.duration.as_millis() as f64)
                .sum::<f64>() / successful_results.len() as f64;
            
            let min_duration = successful_results.iter()
                .map(|r| r.duration.as_millis())
                .min()
                .unwrap_or(0);
            
            let max_duration = successful_results.iter()
                .map(|r| r.duration.as_millis())
                .max()
                .unwrap_or(0);
            
            println!("  {:?}:", provider);
            println!("    Average: {:.0}ms", avg_duration);
            println!("    Min: {}ms", min_duration);
            println!("    Max: {}ms", max_duration);
            println!("    Tests: {}", successful_results.len());
        }
    }
}

/// Get milestone name for display
fn milestone_name(milestone: u8) -> &'static str {
    match milestone {
        1 => "Core Provider Functionality",
        2 => "Tool System Integration",
        3 => "Streaming and Real-time Features",
        4 => "Agentic Event Loop",
        5 => "Telemetry and Observability",
        6 => "Error Handling and Resilience",
        7 => "MCP and Advanced Integration",
        _ => "Unknown Milestone",
    }
}

/// Quick verification runner for CI/CD
pub async fn quick_verification() -> bool {
    println!("‚ö° Quick Verification for CI/CD");
    println!("===============================");
    
    let env = shared::config::TestEnvironment::detect().await;
    if env.available_providers.is_empty() {
        println!("‚ùå No providers available - verification failed");
        return false;
    }
    
    // Run core tests only for available providers
    let mut overall_success = true;
    
    for provider in env.available_providers.iter().take(1) { // Test only first available provider for speed
        let config = env.get_config_for_provider(*provider);
        if let Ok(test_config) = config {
            let suite = shared::test_cases::create_core_test_suite();
            let results = suite.run(&test_config).await;
            
            let success_rate = results.iter().filter(|r| r.success).count() as f64 / results.len() as f64;
            if success_rate < 0.8 { // 80% minimum for quick verification
                overall_success = false;
            }
        }
    }
    
    if overall_success {
        println!("‚úÖ Quick verification PASSED");
    } else {
        println!("‚ùå Quick verification FAILED");
    }
    
    overall_success
}