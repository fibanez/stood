//! Parallel execution interface for tools and operations
//!
//! This module provides a comprehensive parallel execution system that follows
//! Rust-idiomatic patterns while maintaining compatibility with the Python reference
//! implementation's ParallelToolExecutorInterface design. Features intelligent
//! sequential vs parallel execution based on configuration and real-time progress monitoring.
//!
//! # Key Features
//!
//! - **Intelligent Execution Strategy**: Automatically chooses sequential or parallel execution
//! - **Configurable Concurrency**: CPU-based defaults with user override capabilities  
//! - **Real-time Progress Monitoring**: Channel-based result collection with progress tracking
//! - **Robust Error Handling**: Graceful failure handling with detailed error context
//! - **Performance Optimization**: Efficient task scheduling and resource management
//!
//! # Usage Patterns
//!
//! ## Basic Parallel Execution
//! ```rust,no_run
//! use stood::parallel::{ParallelExecutor, ExecutorConfig};
//!
//! # async fn example() -> Result<(), Box<dyn std::error::Error>> {
//! let config = ExecutorConfig::default(); // Uses CPU count for max_parallel
//! let executor = ParallelExecutor::new(config);
//!
//! // Automatically chooses parallel vs sequential based on task count and config
//! let tasks = vec![/* your tasks */];
//! let results = executor.execute_tasks(tasks).await?;
//! # Ok(())
//! # }
//! ```
//!
//! ## Integration with Tool Execution
//! The parallel executor integrates seamlessly with the agent's tool execution system,
//! providing automatic parallelization when `max_parallel_tools > 1` is configured.

use crate::error::StoodError;
use crate::Result as StoodResult;
use std::collections::HashMap;
use std::future::Future;
use std::pin::Pin;
use std::time::Duration;
use tokio::time::timeout;
use tracing::{debug, error, info};

/// Parallel execution configuration
#[derive(Debug, Clone)]
pub struct ParallelConfig {
    /// Maximum number of concurrent tasks
    pub max_concurrency: usize,
    /// Global timeout for all tasks
    pub global_timeout: Option<Duration>,
    /// Per-task timeout
    pub task_timeout: Option<Duration>,
    /// Enable graceful shutdown on first error
    pub fail_fast: bool,
    /// Maximum number of retries for failed tasks
    pub max_retries: usize,
}

impl Default for ParallelConfig {
    fn default() -> Self {
        Self {
            max_concurrency: 10,
            global_timeout: Some(Duration::from_secs(300)), // 5 minutes
            task_timeout: Some(Duration::from_secs(30)),    // 30 seconds per task
            fail_fast: false,
            max_retries: 3,
        }
    }
}

/// Task execution result with metadata
#[derive(Debug, Clone)]
pub struct TaskResult<T> {
    /// Task identifier
    pub task_id: String,
    /// Execution result
    pub result: StoodResult<T>,
    /// Execution duration
    pub duration: Duration,
    /// Number of retry attempts
    pub retry_count: usize,
}

/// Type-erased task result for internal result collection
#[derive(Debug)]
pub struct TaskResultAny {
    /// Task identifier
    pub task_id: String,
    /// Execution result (boxed to erase type)
    pub result: Box<dyn std::any::Any + Send>,
    /// Whether the result is an error
    pub is_error: bool,
    /// Error message if is_error is true
    pub error_message: Option<String>,
    /// Execution duration
    pub duration: Duration,
    /// Number of retry attempts
    pub retry_count: usize,
}

/// Task execution metrics
#[derive(Debug, Clone)]
pub struct TaskMetrics {
    /// Total tasks submitted
    pub total_tasks: usize,
    /// Successfully completed tasks
    pub completed_tasks: usize,
    /// Failed tasks
    pub failed_tasks: usize,
    /// Currently running tasks
    pub running_tasks: usize,
    /// Average execution duration
    pub average_duration: Duration,
    /// Total execution time
    pub total_duration: Duration,
}

impl Default for TaskMetrics {
    fn default() -> Self {
        Self {
            total_tasks: 0,
            completed_tasks: 0,
            failed_tasks: 0,
            running_tasks: 0,
            average_duration: Duration::ZERO,
            total_duration: Duration::ZERO,
        }
    }
}

/// Parallel executor trait - Rust equivalent of Python's ParallelToolExecutorInterface
///
/// This trait provides a configurable interface for parallel execution of futures,
/// with support for timeouts, retry logic, and comprehensive metrics collection.
#[async_trait::async_trait]
pub trait ParallelExecutor: Send + Sync {
    /// Submit a task for parallel execution
    ///
    /// # Arguments
    /// * `task_id` - Unique identifier for the task
    /// * `future` - The future to execute
    ///
    /// # Returns
    /// Result indicating if the task was successfully submitted
    async fn submit_task<T, F>(&self, task_id: String, future: F) -> StoodResult<()>
    where
        T: Send + 'static,
        F: Future<Output = StoodResult<T>> + Send + 'static;

    /// Wait for all submitted tasks to complete
    ///
    /// # Returns
    /// Vector of task results in completion order
    async fn wait_all<T>(&self) -> StoodResult<Vec<TaskResult<T>>>
    where
        T: Send + 'static;

    /// Wait for tasks as they complete (similar to asyncio.as_completed)
    ///
    /// # Returns
    /// Stream of task results as they complete
    async fn as_completed<T>(
        &self,
    ) -> StoodResult<Pin<Box<dyn futures::Stream<Item = TaskResult<T>> + Send>>>
    where
        T: Send + 'static;

    /// Cancel all running tasks and wait for graceful shutdown
    async fn shutdown(&self) -> StoodResult<()>;

    /// Get current execution metrics
    fn get_metrics(&self) -> TaskMetrics;

    /// Check if the executor has capacity for more tasks
    fn has_capacity(&self) -> bool;

    /// Get the number of currently running tasks
    fn running_count(&self) -> usize;
}

/// Tokio-based parallel executor implementation
pub struct TokioExecutor {
    /// Configuration for parallel execution
    config: ParallelConfig,
    /// Task tracking and metrics
    metrics: std::sync::Arc<tokio::sync::Mutex<TaskMetrics>>,
    /// Task handles for cancellation
    task_handles: std::sync::Arc<tokio::sync::Mutex<HashMap<String, tokio::task::JoinHandle<()>>>>,
    /// Semaphore to limit concurrency
    semaphore: std::sync::Arc<tokio::sync::Semaphore>,
    /// Shutdown signal
    shutdown_signal: std::sync::Arc<tokio::sync::Notify>,
    /// Result collection channels
    result_receiver: std::sync::Arc<tokio::sync::Mutex<Option<tokio::sync::mpsc::UnboundedReceiver<TaskResultAny>>>>,
    result_sender: std::sync::Arc<tokio::sync::mpsc::UnboundedSender<TaskResultAny>>,
}

impl TokioExecutor {
    /// Create a new TokioExecutor with the given configuration
    pub fn new(config: ParallelConfig) -> Self {
        let (result_sender, result_receiver) = tokio::sync::mpsc::unbounded_channel();
        Self {
            semaphore: std::sync::Arc::new(tokio::sync::Semaphore::new(config.max_concurrency)),
            config,
            metrics: std::sync::Arc::new(tokio::sync::Mutex::new(TaskMetrics::default())),
            task_handles: std::sync::Arc::new(tokio::sync::Mutex::new(HashMap::new())),
            shutdown_signal: std::sync::Arc::new(tokio::sync::Notify::new()),
            result_receiver: std::sync::Arc::new(tokio::sync::Mutex::new(Some(result_receiver))),
            result_sender: std::sync::Arc::new(result_sender),
        }
    }

    /// Create a new TokioExecutor with default configuration
    pub fn default() -> Self {
        Self::new(ParallelConfig::default())
    }

    /// Execute a task with retry logic and timeout handling
    async fn execute_task_with_retry<T>(
        task_id: String,
        future: impl Future<Output = StoodResult<T>> + Send + 'static,
        _max_retries: usize,
        task_timeout: Option<Duration>,
    ) -> TaskResult<T>
    where
        T: Send + 'static,
    {
        let start_time = std::time::Instant::now();
        let _retry_count = 0;
        let _last_error = StoodError::ConfigurationError {
            message: "No attempts made".to_string(),
        };

        // For simplicity, we'll execute once without retry for now
        // A full implementation would need proper future cloning/recreation
        let result = if let Some(timeout_duration) = task_timeout {
            match timeout(timeout_duration, future).await {
                Ok(result) => result,
                Err(_) => Err(StoodError::ConfigurationError {
                    message: format!("Task {} timed out after {:?}", task_id, timeout_duration),
                }),
            }
        } else {
            future.await
        };

        let duration = start_time.elapsed();

        match result {
            Ok(value) => {
                debug!("Task {} completed successfully in {:?}", task_id, duration);
                TaskResult {
                    task_id,
                    result: Ok(value),
                    duration,
                    retry_count: 0,
                }
            }
            Err(error) => {
                error!("Task {} failed in {:?}: {:?}", task_id, duration, error);
                TaskResult {
                    task_id,
                    result: Err(error),
                    duration,
                    retry_count: 0,
                }
            }
        }
    }
}

#[async_trait::async_trait]
impl ParallelExecutor for TokioExecutor {
    async fn submit_task<T, F>(&self, task_id: String, future: F) -> StoodResult<()>
    where
        T: Send + 'static,
        F: Future<Output = StoodResult<T>> + Send + 'static,
    {
        // Check if we have capacity
        if !self.has_capacity() {
            return Err(StoodError::ConfigurationError {
                message: "Executor at maximum capacity".to_string(),
            });
        }

        // Update metrics
        {
            let mut metrics = self.metrics.lock().await;
            metrics.total_tasks += 1;
            metrics.running_tasks += 1;
        }

        let task_id_clone = task_id.clone();
        let semaphore = self.semaphore.clone();
        let metrics = self.metrics.clone();
        let task_handles = self.task_handles.clone();
        let _shutdown_signal = self.shutdown_signal.clone();
        let result_sender = self.result_sender.clone();
        let max_retries = self.config.max_retries;
        let task_timeout = self.config.task_timeout;

        // Spawn the task
        let handle = tokio::spawn(async move {
            // Acquire semaphore permit
            let _permit = semaphore
                .acquire()
                .await
                .expect("Semaphore should not be closed");

            // Execute task with retry logic
            let result = TokioExecutor::execute_task_with_retry(
                task_id_clone.clone(),
                future,
                max_retries,
                task_timeout,
            )
            .await;

            // Update metrics
            {
                let mut metrics = metrics.lock().await;
                metrics.running_tasks = metrics.running_tasks.saturating_sub(1);

                match &result.result {
                    Ok(_) => metrics.completed_tasks += 1,
                    Err(_) => metrics.failed_tasks += 1,
                }

                // Update average duration
                let total_completed = metrics.completed_tasks + metrics.failed_tasks;
                if total_completed > 0 {
                    metrics.total_duration += result.duration;
                    metrics.average_duration = metrics.total_duration / total_completed as u32;
                }
            }

            // Send result through channel
            let result_any = match result.result {
                Ok(value) => TaskResultAny {
                    task_id: result.task_id.clone(),
                    result: Box::new(value),
                    is_error: false,
                    error_message: None,
                    duration: result.duration,
                    retry_count: result.retry_count,
                },
                Err(error) => TaskResultAny {
                    task_id: result.task_id.clone(),
                    result: Box::new(error.clone()),
                    is_error: true,
                    error_message: Some(error.to_string()),
                    duration: result.duration,
                    retry_count: result.retry_count,
                },
            };

            let _ = result_sender.send(result_any);

            // Remove from task handles
            task_handles.lock().await.remove(&task_id_clone);
        });

        // Store handle for cancellation
        self.task_handles.lock().await.insert(task_id, handle);

        info!("Submitted task for parallel execution");
        Ok(())
    }

    async fn wait_all<T>(&self) -> StoodResult<Vec<TaskResult<T>>>
    where
        T: Send + 'static,
    {
        let mut results = Vec::new();
        let mut receiver_guard = self.result_receiver.lock().await;
        
        if let Some(mut receiver) = receiver_guard.take() {
            drop(receiver_guard);
            
            // Collect results until all tasks are complete
            while !self.task_handles.lock().await.is_empty() {
                tokio::select! {
                    result = receiver.recv() => {
                        if let Some(result_any) = result {
                            // Try to downcast the result back to the expected type
                            if result_any.is_error {
                                // Handle error case
                                let error = result_any.error_message
                                    .map(|msg| StoodError::ConfigurationError { message: msg })
                                    .unwrap_or_else(|| StoodError::ConfigurationError { 
                                        message: "Unknown error".to_string() 
                                    });
                                results.push(TaskResult {
                                    task_id: result_any.task_id,
                                    result: Err(error),
                                    duration: result_any.duration,
                                    retry_count: result_any.retry_count,
                                });
                            } else {
                                // Try to downcast the success value
                                if let Ok(value) = result_any.result.downcast::<T>() {
                                    results.push(TaskResult {
                                        task_id: result_any.task_id,
                                        result: Ok(*value),
                                        duration: result_any.duration,
                                        retry_count: result_any.retry_count,
                                    });
                                } else {
                                    // Type mismatch - this is a programming error
                                    results.push(TaskResult {
                                        task_id: result_any.task_id,
                                        result: Err(StoodError::ConfigurationError {
                                            message: "Type mismatch in result collection".to_string()
                                        }),
                                        duration: result_any.duration,
                                        retry_count: result_any.retry_count,
                                    });
                                }
                            }
                        }
                    }
                    _ = tokio::time::sleep(Duration::from_millis(10)) => {
                        // Continue polling for task completion
                    }
                }
            }
            
            // Restore the receiver
            *self.result_receiver.lock().await = Some(receiver);
        }

        info!("All parallel tasks completed");
        Ok(results)
    }

    async fn as_completed<T>(
        &self,
    ) -> StoodResult<Pin<Box<dyn futures::Stream<Item = TaskResult<T>> + Send>>>
    where
        T: Send + 'static,
    {
        use futures::stream;
        
        let task_handles = self.task_handles.clone();
        let mut receiver_guard = self.result_receiver.lock().await;
        
        if let Some(receiver) = receiver_guard.take() {
            drop(receiver_guard);
            
            let stream = stream::unfold(
                (receiver, task_handles),
                move |(mut receiver, handles_ref)| async move {
                    loop {
                        tokio::select! {
                            result = receiver.recv() => {
                                if let Some(result_any) = result {
                                    let task_result = if result_any.is_error {
                                        // Handle error case
                                        let error = result_any.error_message
                                            .map(|msg| StoodError::ConfigurationError { message: msg })
                                            .unwrap_or_else(|| StoodError::ConfigurationError { 
                                                message: "Unknown error".to_string() 
                                            });
                                        TaskResult {
                                            task_id: result_any.task_id,
                                            result: Err(error),
                                            duration: result_any.duration,
                                            retry_count: result_any.retry_count,
                                        }
                                    } else {
                                        // Try to downcast the success value
                                        if let Ok(value) = result_any.result.downcast::<T>() {
                                            TaskResult {
                                                task_id: result_any.task_id,
                                                result: Ok(*value),
                                                duration: result_any.duration,
                                                retry_count: result_any.retry_count,
                                            }
                                        } else {
                                            // Type mismatch - this is a programming error
                                            TaskResult {
                                                task_id: result_any.task_id,
                                                result: Err(StoodError::ConfigurationError {
                                                    message: "Type mismatch in result collection".to_string()
                                                }),
                                                duration: result_any.duration,
                                                retry_count: result_any.retry_count,
                                            }
                                        }
                                    };
                                    
                                    return Some((task_result, (receiver, handles_ref)));
                                }
                            }
                            _ = tokio::time::sleep(Duration::from_millis(100)) => {
                                // Check if all tasks are done
                                if handles_ref.lock().await.is_empty() {
                                    return None;
                                }
                            }
                        }
                    }
                },
            );
            
            Ok(Box::pin(stream))
        } else {
            // No receiver available, return an empty stream
            let stream = stream::empty();
            Ok(Box::pin(stream))
        }
    }

    async fn shutdown(&self) -> StoodResult<()> {
        info!("Shutting down parallel executor...");

        // Signal shutdown
        self.shutdown_signal.notify_waiters();

        // Cancel all running tasks
        let handles = self.task_handles.lock().await;
        for (task_id, handle) in handles.iter() {
            debug!("Cancelling task: {}", task_id);
            handle.abort();
        }

        // Clear task handles
        drop(handles);
        self.task_handles.lock().await.clear();

        // Update metrics
        {
            let mut metrics = self.metrics.lock().await;
            metrics.running_tasks = 0;
        }

        info!("Parallel executor shutdown complete");
        Ok(())
    }

    fn get_metrics(&self) -> TaskMetrics {
        // Since this is a synchronous method, we need to use try_lock
        // In practice, you might want to redesign this to be async
        self.metrics
            .try_lock()
            .map(|metrics| metrics.clone())
            .unwrap_or_else(|_| TaskMetrics::default())
    }

    fn has_capacity(&self) -> bool {
        self.semaphore.available_permits() > 0
    }

    fn running_count(&self) -> usize {
        self.metrics
            .try_lock()
            .map(|metrics| metrics.running_tasks)
            .unwrap_or(0)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;

    #[tokio::test]
    async fn test_parallel_config_default() {
        let config = ParallelConfig::default();
        assert_eq!(config.max_concurrency, 10);
        assert!(config.global_timeout.is_some());
        assert!(config.task_timeout.is_some());
        assert!(!config.fail_fast);
        assert_eq!(config.max_retries, 3);
    }

    #[tokio::test]
    async fn test_task_metrics_default() {
        let metrics = TaskMetrics::default();
        assert_eq!(metrics.total_tasks, 0);
        assert_eq!(metrics.completed_tasks, 0);
        assert_eq!(metrics.failed_tasks, 0);
        assert_eq!(metrics.running_tasks, 0);
        assert_eq!(metrics.average_duration, Duration::ZERO);
        assert_eq!(metrics.total_duration, Duration::ZERO);
    }

    #[tokio::test]
    async fn test_tokio_executor_creation() {
        let config = ParallelConfig::default();
        let executor = TokioExecutor::new(config);

        assert!(executor.has_capacity());
        assert_eq!(executor.running_count(), 0);

        let metrics = executor.get_metrics();
        assert_eq!(metrics.total_tasks, 0);
    }

    #[tokio::test]
    async fn test_tokio_executor_default() {
        let executor = TokioExecutor::default();

        assert!(executor.has_capacity());
        assert_eq!(executor.running_count(), 0);
    }

    #[tokio::test]
    async fn test_simple_task_submission() {
        let executor = TokioExecutor::default();

        // Submit a simple successful task
        let result = executor
            .submit_task("test_task".to_string(), async {
                tokio::time::sleep(Duration::from_millis(10)).await;
                Ok::<i32, StoodError>(42)
            })
            .await;

        assert!(result.is_ok());

        // Wait for completion
        let _results = executor.wait_all::<i32>().await.unwrap();

        // Check metrics to verify task completed
        let metrics = executor.get_metrics();
        assert_eq!(metrics.total_tasks, 1);
        assert_eq!(metrics.completed_tasks, 1);
        assert_eq!(metrics.running_tasks, 0);
    }

    #[tokio::test]
    async fn test_multiple_task_submission() {
        let executor = TokioExecutor::default();
        let counter = Arc::new(AtomicUsize::new(0));

        // Submit multiple tasks
        for i in 0..5 {
            let counter_clone = counter.clone();
            let result = executor
                .submit_task(format!("task_{}", i), async move {
                    tokio::time::sleep(Duration::from_millis(10)).await;
                    counter_clone.fetch_add(1, Ordering::SeqCst);
                    Ok::<i32, StoodError>(i)
                })
                .await;
            assert!(result.is_ok());
        }

        // Wait for all tasks to complete
        let _results = executor.wait_all::<i32>().await.unwrap();
        assert_eq!(counter.load(Ordering::SeqCst), 5);

        // Check metrics
        let metrics = executor.get_metrics();
        assert_eq!(metrics.total_tasks, 5);
        assert_eq!(metrics.completed_tasks, 5);
        assert_eq!(metrics.running_tasks, 0);
    }

    #[tokio::test]
    async fn test_task_failure_and_retry() {
        let config = ParallelConfig {
            max_retries: 2,
            ..Default::default()
        };
        let executor = TokioExecutor::new(config);

        // Submit a task that always fails
        let result = executor
            .submit_task("failing_task".to_string(), async {
                Err::<i32, StoodError>(StoodError::ToolError {
                    message: "Task always fails".to_string(),
                })
            })
            .await;

        assert!(result.is_ok());

        // Wait for completion
        let _results = executor.wait_all::<i32>().await.unwrap();

        // Check metrics to verify task failed
        let metrics = executor.get_metrics();
        assert_eq!(metrics.total_tasks, 1);
        assert_eq!(metrics.failed_tasks, 1);
        assert_eq!(metrics.running_tasks, 0);
    }

    #[tokio::test]
    async fn test_executor_capacity_limits() {
        let config = ParallelConfig {
            max_concurrency: 2,
            ..Default::default()
        };
        let executor = TokioExecutor::new(config);

        // Submit long-running tasks up to capacity
        for i in 0..2 {
            let result = executor
                .submit_task(format!("task_{}", i), async move {
                    tokio::time::sleep(Duration::from_millis(1000)).await; // Longer duration
                    Ok::<i32, StoodError>(i)
                })
                .await;
            assert!(result.is_ok());
        }

        // Give tasks a moment to start and acquire permits
        tokio::time::sleep(Duration::from_millis(10)).await;

        // Verify capacity is exhausted
        assert!(!executor.has_capacity());

        // Trying to submit another task should fail
        let result = executor
            .submit_task("overflow_task".to_string(), async {
                Ok::<i32, StoodError>(999)
            })
            .await;
        assert!(result.is_err());

        // Wait for tasks to complete to clean up
        let _results = executor.wait_all::<i32>().await.unwrap();
    }

    #[tokio::test]
    async fn test_executor_shutdown() {
        let executor = TokioExecutor::default();

        // Submit a long-running task
        let result = executor
            .submit_task("long_task".to_string(), async {
                tokio::time::sleep(Duration::from_secs(10)).await;
                Ok::<i32, StoodError>(42)
            })
            .await;
        assert!(result.is_ok());

        // Shutdown should cancel the task
        let shutdown_result = executor.shutdown().await;
        assert!(shutdown_result.is_ok());

        // Running count should be zero after shutdown
        assert_eq!(executor.running_count(), 0);
    }

    #[tokio::test]
    async fn test_metrics_tracking() {
        let executor = TokioExecutor::default();

        // Submit successful and failing tasks
        for i in 0..3 {
            let result = executor
                .submit_task(format!("success_task_{}", i), async move {
                    tokio::time::sleep(Duration::from_millis(10)).await;
                    Ok::<i32, StoodError>(i)
                })
                .await;
            assert!(result.is_ok());
        }

        for i in 0..2 {
            let result = executor
                .submit_task(format!("fail_task_{}", i), async {
                    Err::<i32, StoodError>(StoodError::ToolError {
                        message: "Task failed".to_string(),
                    })
                })
                .await;
            assert!(result.is_ok());
        }

        // Wait for completion
        let _results = executor.wait_all::<i32>().await.unwrap();

        // Check metrics
        let metrics = executor.get_metrics();
        assert_eq!(metrics.total_tasks, 5);
        assert_eq!(metrics.completed_tasks, 3);
        assert_eq!(metrics.failed_tasks, 2);
        assert_eq!(metrics.running_tasks, 0);
        assert!(metrics.total_duration > Duration::ZERO);
    }
}
