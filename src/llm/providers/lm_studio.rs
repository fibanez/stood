//! LM Studio provider implementation.
//!
//! This provider connects to a local LM Studio instance via HTTP API
//! and handles OpenAI-compatible request/response formatting.

use crate::llm::traits::{LlmProvider, ProviderType, LlmError, ChatResponse, ChatConfig, Tool, StreamEvent, ProviderCapabilities, HealthStatus};
use crate::llm::providers::retry::{RetryConfig, retry_llm_operation, BoxFuture};
use crate::types::{Messages, MessageRole, ContentBlock};
use async_trait::async_trait;
use futures::Stream;
use serde_json::Value;
use std::time::Instant;

/// Tool state management for LM Studio streaming (following Claude's pattern)
#[derive(Debug)]
struct LMStudioToolState {
    /// Currently active tool calls being assembled (supports multiple parallel tools)
    active_tool_calls: std::collections::HashMap<String, crate::llm::traits::ToolCall>,
    /// Input buffers for each tool call (tool_id -> accumulated JSON string)  
    tool_input_buffers: std::collections::HashMap<String, String>,
}

impl LMStudioToolState {
    fn new() -> Self {
        Self {
            active_tool_calls: std::collections::HashMap::new(),
            tool_input_buffers: std::collections::HashMap::new(),
        }
    }

    fn reset(&mut self) {
        self.active_tool_calls.clear();
        self.tool_input_buffers.clear();
    }

    fn start_tool_call(&mut self, tool_id: String, name: String) {
        let tool_call = crate::llm::traits::ToolCall {
            id: tool_id.clone(),
            name,
            input: serde_json::Value::Null,
        };
        self.active_tool_calls.insert(tool_id.clone(), tool_call);
        self.tool_input_buffers.insert(tool_id, String::new());
    }

    fn add_input_delta(&mut self, tool_id: &str, delta: &str) {
        if let Some(buffer) = self.tool_input_buffers.get_mut(tool_id) {
            buffer.push_str(delta);
            
            // Try to parse accumulated input as complete JSON
            if let Ok(parsed_input) = serde_json::from_str::<serde_json::Value>(buffer) {
                if let Some(tool_call) = self.active_tool_calls.get_mut(tool_id) {
                    tool_call.input = parsed_input;
                }
            }
        }
    }


    fn get_all_complete_tools(&mut self) -> Vec<crate::llm::traits::ToolCall> {
        // Return all tools that have valid JSON input
        let complete_tools: Vec<_> = self.active_tool_calls.values()
            .filter(|tool| !tool.input.is_null())
            .cloned()
            .collect();
        
        if !complete_tools.is_empty() {
            self.reset();
        }
        
        complete_tools
    }
}

/// LM Studio provider
/// 
/// This provider connects to a local LM Studio instance and handles
/// OpenAI-compatible API formatting for local models.
/// 
/// Includes retry logic with exponential backoff to handle model loading delays.
#[derive(Debug)]
pub struct LMStudioProvider {
    base_url: String,
    client: reqwest::Client,
    retry_config: RetryConfig,
}


impl LMStudioProvider {
    /// Create a new LM Studio provider with default retry configuration
    pub async fn new(base_url: String) -> Result<Self, LlmError> {
        Self::with_retry_config(base_url, RetryConfig::lm_studio_default()).await
    }
    
    /// Create a new LM Studio provider with custom retry configuration
    pub async fn with_retry_config(base_url: String, retry_config: RetryConfig) -> Result<Self, LlmError> {
        let client = reqwest::Client::new();
        
        // TODO: Test connection to LM Studio with retry logic
        
        Ok(Self {
            base_url,
            client,
            retry_config,
        })
    }
    
    /// Get the current retry configuration
    pub fn retry_config(&self) -> &RetryConfig {
        &self.retry_config
    }
    
    /// Update the retry configuration
    pub fn set_retry_config(&mut self, config: RetryConfig) {
        self.retry_config = config;
    }
    
    /// Make a retryable HTTP request to LM Studio
    async fn make_retryable_request(
        &self,
        endpoint: &str,
        request_body: serde_json::Value,
    ) -> Result<String, LlmError> {
        let url = format!("{}{}", self.base_url, endpoint);
        tracing::trace!("üîµ LM Studio POST to: {}", url);
        tracing::trace!("üîµ LM Studio POST body: {}", serde_json::to_string_pretty(&request_body).unwrap_or_else(|_| "<invalid json>".to_string()));
        
        // Clone necessary data for retry closure
        let client = self.client.clone();
        let url_clone = url.clone();
        let request_body_clone = request_body.clone();
        
        retry_llm_operation(
            move || {
                let client = client.clone();
                let url = url_clone.clone();
                let request_body = request_body_clone.clone();
                
                Box::pin(async move {
                    // Make HTTP request
                    let response = client
                        .post(&url)
                        .header("Content-Type", "application/json")
                        .json(&request_body)
                        .send()
                        .await
                        .map_err(|e| {
                            tracing::debug!("üîÑ LM Studio HTTP request attempt failed: {}", e);
                            LlmError::ProviderError {
                                provider: ProviderType::LmStudio,
                                message: format!("HTTP request failed: {}", e),
                                source: Some(Box::new(e)),
                            }
                        })?;
                    
                    let status = response.status();
                    
                    // Check if response indicates a retryable condition
                    if !status.is_success() {
                        let error_text = response.text().await.unwrap_or_else(|_| "Unknown error".to_string());
                        
                        // Retryable errors for LM Studio (model loading, startup issues)
                        let _retryable_error = match status.as_u16() {
                            502 | 503 => {
                                tracing::warn!("üîÑ LM Studio returned {} (model may be loading): {}", status, error_text);
                                true
                            }
                            _ => {
                                tracing::error!("‚ùå LM Studio API error {} (non-retryable): {}", status, error_text);
                                false
                            }
                        };
                        
                        return Err(LlmError::ProviderError {
                            provider: ProviderType::LmStudio,
                            message: format!("LM Studio API error {}: {}", status, error_text),
                            source: None,
                        });
                    }
                    
                    // Parse response text
                    response.text().await.map_err(|e| {
                        tracing::error!("‚ùå Failed to read response text: {}", e);
                        LlmError::ProviderError {
                            provider: ProviderType::LmStudio,
                            message: format!("Failed to read response text: {}", e),
                            source: Some(Box::new(e)),
                        }
                    })
                }) as BoxFuture<'_, Result<String, LlmError>>
            },
            &self.retry_config,
        ).await
    }
}

#[async_trait]
impl LlmProvider for LMStudioProvider {
    async fn chat(
        &self,
        model_id: &str,
        messages: &Messages,
        config: &ChatConfig,
    ) -> Result<ChatResponse, LlmError> {
        tracing::info!("üîµ LM Studio chat request starting for model: {}", model_id);
        tracing::debug!("üîµ LM Studio request config: temp={:?}, max_tokens={:?}", 
            config.temperature, config.max_tokens);
        
        // Convert Messages to OpenAI format
        let openai_messages = self.convert_messages_to_openai(messages)?;
        tracing::debug!("üîµ Converted {} messages to OpenAI format", openai_messages.len());
        
        // Build OpenAI-compatible request
        let mut request_body = serde_json::json!({
            "model": model_id,
            "messages": openai_messages,
            "max_tokens": config.max_tokens.unwrap_or(1000),
            "temperature": config.temperature.unwrap_or(0.7),
            "stream": false
        });
        
        // Add additional parameters if present
        for (key, value) in &config.additional_params {
            request_body[key] = value.clone();
        }
        
        // Make HTTP request to LM Studio with retry logic
        let start_time = std::time::Instant::now();
        let response_text = self.make_retryable_request("/v1/chat/completions", request_body).await?;
        let request_duration = start_time.elapsed();
        tracing::debug!("üîµ LM Studio HTTP request completed in {:?}", request_duration);
        
        tracing::trace!("üîµ LM Studio raw response: {}", response_text);
        
        let response_json: serde_json::Value = serde_json::from_str(&response_text)
            .map_err(|e| {
                tracing::error!("‚ùå Failed to parse JSON response: {} | Raw response: {}", e, response_text);
                LlmError::ProviderError {
                    provider: ProviderType::LmStudio,
                    message: format!("Failed to parse JSON response: {}", e),
                    source: Some(Box::new(e)),
                }
            })?;
        
        tracing::debug!("üîµ LM Studio parsed JSON response structure: {}", 
            serde_json::to_string(&response_json).unwrap_or_else(|_| "<invalid>".to_string()));
        
        // Convert OpenAI response to ChatResponse
        let chat_response = self.convert_openai_response_to_chat_response(response_json)?;
        
        tracing::info!("‚úÖ LM Studio chat response completed in {:?}, content length: {} chars", 
            start_time.elapsed(), chat_response.content.len());
        tracing::debug!("üîµ LM Studio response content: {}", chat_response.content);
        
        Ok(chat_response)
    }
    
    async fn chat_with_tools(
        &self,
        model_id: &str,
        messages: &Messages,
        tools: &[Tool],
        config: &ChatConfig,
    ) -> Result<ChatResponse, LlmError> {
        if tools.is_empty() {
            // No tools, use basic chat
            return self.chat(model_id, messages, config).await;
        }

        tracing::info!("üîß LM Studio chat with {} tools for model: {}", tools.len(), model_id);
        let start_time = Instant::now();

        // Convert Messages to OpenAI format
        let openai_messages = self.convert_messages_to_openai(messages)?;
        
        // Convert tools to OpenAI tool format
        let openai_tools: Vec<serde_json::Value> = tools.iter().map(|tool| {
            serde_json::json!({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.input_schema
                }
            })
        }).collect();

        // Build OpenAI-compatible request with tools
        let mut request_body = serde_json::json!({
            "model": model_id,
            "messages": openai_messages,
            "tools": openai_tools,
            "tool_choice": "auto", // Let the model decide when to use tools
            "temperature": config.temperature.unwrap_or(0.7),
            "max_tokens": config.max_tokens,
        });

        // Add additional parameters if present in config
        for (key, value) in &config.additional_params {
            request_body[key] = value.clone();
        }

        tracing::debug!("üîµ LM Studio request with tools: {}", 
            serde_json::to_string_pretty(&request_body).unwrap_or_else(|_| "<invalid>".to_string()));

        // Make HTTP request to LM Studio with retry logic
        let response_text = self.make_retryable_request("/v1/chat/completions", request_body).await?;

        let response_json: serde_json::Value = serde_json::from_str(&response_text)
            .map_err(|e| LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: format!("Failed to parse LM Studio JSON: {}", e),
                source: Some(Box::new(e)),
            })?;

        tracing::debug!("üîµ LM Studio response with tools: {}", 
            serde_json::to_string_pretty(&response_json).unwrap_or_else(|_| "<invalid>".to_string()));

        // Convert OpenAI response to ChatResponse
        let chat_response = self.convert_openai_response_to_chat_response_with_tools(response_json)?;

        // Debug log to show parsed tool calls
        if !chat_response.tool_calls.is_empty() {
            tracing::debug!("üîß LM Studio parsed tool calls:");
            for (i, tool_call) in chat_response.tool_calls.iter().enumerate() {
                tracing::debug!("  Tool {}: {} - args: {}", 
                    i + 1, 
                    tool_call.name,
                    serde_json::to_string(&tool_call.input).unwrap_or_else(|_| "<invalid>".to_string())
                );
            }
        }

        tracing::info!("‚úÖ LM Studio chat with tools completed in {:?}, content length: {} chars, tool calls: {}", 
            start_time.elapsed(), chat_response.content.len(), chat_response.tool_calls.len());

        Ok(chat_response)
    }
    
    async fn chat_streaming(
        &self,
        model_id: &str,
        messages: &Messages,
        config: &ChatConfig,
    ) -> Result<Box<dyn Stream<Item = StreamEvent> + Send + Unpin>, LlmError> {
        tracing::info!("üåä LM Studio streaming request starting for model: {}", model_id);
        
        // Convert Messages to OpenAI format
        let openai_messages = self.convert_messages_to_openai(messages)?;
        
        // Build OpenAI-compatible streaming request
        let mut request_body = serde_json::json!({
            "model": model_id,
            "messages": openai_messages,
            "max_tokens": config.max_tokens.unwrap_or(1000),
            "temperature": config.temperature.unwrap_or(0.7),
            "stream": true  // Enable streaming
        });
        
        // Add additional parameters if present
        for (key, value) in &config.additional_params {
            request_body[key] = value.clone();
        }
        
        let url = format!("{}/v1/chat/completions", self.base_url);
        tracing::trace!("üåä LM Studio streaming POST to: {}", url);
        tracing::trace!("üåä LM Studio streaming POST body: {}", serde_json::to_string_pretty(&request_body).unwrap_or_else(|_| "<invalid json>".to_string()));
        
        // JSON ONLY output for debug_json_only example
        if std::env::var("RUST_LOG").unwrap_or_default().contains("debug_json_only") {
            println!("OUT {}", serde_json::to_string(&request_body).unwrap_or_default());
        }
        
        // Make streaming HTTP request
        let response = self.client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("Accept", "text/event-stream")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| {
                tracing::error!("‚ùå LM Studio streaming request failed: {}", e);
                LlmError::ProviderError {
                    provider: ProviderType::LmStudio,
                    message: format!("Streaming request failed: {}", e),
                    source: Some(Box::new(e)),
                }
            })?;
        
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_else(|_| "Unknown error".to_string());
            tracing::error!("‚ùå LM Studio streaming API error {}: {}", status, error_text);
            return Err(LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: format!("LM Studio streaming API error {}: {}", status, error_text),
                source: None,
            });
        }
        
        tracing::debug!("üåä LM Studio streaming response received, processing SSE stream...");
        
        // Convert the response to a stream of events
        let stream = self.parse_sse_stream(response).await?;
        Ok(stream)
    }
    
    async fn chat_streaming_with_tools(
        &self,
        model_id: &str,
        messages: &Messages,
        tools: &[Tool],
        config: &ChatConfig,
    ) -> Result<Box<dyn Stream<Item = StreamEvent> + Send + Unpin>, LlmError> {
        tracing::info!("üîßüåä LM Studio streaming with tools request starting for model: {} with {} tools", model_id, tools.len());
        
        // Convert Messages to OpenAI format
        let openai_messages = self.convert_messages_to_openai(messages)?;
        
        // Debug: Log the conversation being sent to LM Studio
        tracing::debug!("üîß LM Studio conversation has {} messages:", openai_messages.len());
        for (i, msg) in openai_messages.iter().enumerate() {
            tracing::debug!("üîß Message {}: {:?}", i, msg);
        }
        
        // Convert tools to OpenAI format
        let openai_tools = self.convert_tools_to_openai(tools)?;
        
        // Build OpenAI-compatible streaming request with tools
        let mut request_body = serde_json::json!({
            "model": model_id,
            "messages": openai_messages,
            "max_tokens": config.max_tokens.unwrap_or(1000),
            "temperature": config.temperature.unwrap_or(0.7),
            "stream": true,  // Enable streaming
            "tools": openai_tools  // Include tools
        });
        
        // Add additional parameters if present
        for (key, value) in &config.additional_params {
            request_body[key] = value.clone();
        }
        
        let url = format!("{}/v1/chat/completions", self.base_url);
        tracing::debug!("üîßüåä LM Studio streaming with tools POST to: {}", url);
        tracing::debug!("üîßüåä LM Studio streaming with tools REQUEST:");
        tracing::debug!("-----------------------------------------------------------");
        tracing::debug!("{}", serde_json::to_string_pretty(&request_body).unwrap_or_else(|_| "<invalid json>".to_string()));
        tracing::debug!("-----------------------------------------------------------");
        
        // JSON ONLY output for debug_json_only example
        if std::env::var("RUST_LOG").unwrap_or_default().contains("debug_json_only") {
            println!("OUT {}", serde_json::to_string(&request_body).unwrap_or_default());
        }
        
        // Make streaming HTTP request
        let response = self.client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("Accept", "text/event-stream")
            .json(&request_body)
            .send()
            .await
            .map_err(|e| {
                tracing::error!("‚ùå LM Studio streaming with tools request failed: {}", e);
                LlmError::ProviderError {
                    provider: ProviderType::LmStudio,
                    message: format!("Streaming with tools request failed: {}", e),
                    source: Some(Box::new(e)),
                }
            })?;
        
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_else(|_| "Unknown error".to_string());
            tracing::error!("‚ùå LM Studio streaming with tools API error {}: {}", status, error_text);
            return Err(LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: format!("LM Studio streaming with tools API error {}: {}", status, error_text),
                source: None,
            });
        }
        
        tracing::debug!("üîßüåä LM Studio streaming with tools response received, processing SSE stream...");
        
        // Convert the response to a stream of events (same parsing logic can handle tools)
        let stream = self.parse_sse_stream(response).await?;
        Ok(stream)
    }
    
    async fn health_check(&self) -> Result<HealthStatus, LlmError> {
        // Test connection to LM Studio with conservative retry (1 retry for connection failures only)
        let url = format!("{}/v1/models", self.base_url);
        
        let start = std::time::Instant::now();
        let conservative_retry = RetryConfig {
            max_attempts: 1,  // Only 1 retry for health checks
            initial_delay: std::time::Duration::from_millis(500),
            max_delay: std::time::Duration::from_millis(1000),
            backoff_multiplier: 2.0,
            jitter: false,
        };
        
        let client = self.client.clone();
        let url_clone = url.clone();
        
        let result = retry_llm_operation(
            move || {
                let client = client.clone();
                let url = url_clone.clone();
                
                Box::pin(async move {
                    client.get(&url).send().await.map_err(|e| {
                        // Only retry on connection errors, not HTTP status errors
                        if e.is_connect() || e.is_timeout() {
                            tracing::debug!("üîÑ Health check connection failed (retryable): {}", e);
                        } else {
                            tracing::debug!("üîÑ Health check failed (non-retryable): {}", e);
                        }
                        LlmError::NetworkError {
                            message: format!("Health check request failed: {}", e),
                            source: Some(Box::new(e)),
                        }
                    })
                }) as BoxFuture<'_, Result<reqwest::Response, LlmError>>
            },
            &conservative_retry,
        ).await;
        
        let latency = start.elapsed().as_millis() as u64;
        
        match result {
            Ok(response) if response.status().is_success() => {
                Ok(HealthStatus {
                    healthy: true,
                    provider: ProviderType::LmStudio,
                    latency_ms: Some(latency),
                    error: None,
                })
            }
            Ok(response) => {
                Ok(HealthStatus {
                    healthy: false,
                    provider: ProviderType::LmStudio,
                    latency_ms: Some(latency),
                    error: Some(format!("HTTP {}: {}", response.status(), response.status().canonical_reason().unwrap_or("Unknown"))),
                })
            }
            Err(e) => {
                Ok(HealthStatus {
                    healthy: false,
                    provider: ProviderType::LmStudio,
                    latency_ms: None,
                    error: Some(format!("Connection failed: {}", e)),
                })
            }
        }
    }
    
    fn capabilities(&self) -> ProviderCapabilities {
        ProviderCapabilities {
            supports_streaming: true,
            supports_tools: false, // Depends on model and LM Studio configuration
            supports_thinking: false,
            supports_vision: false, // Depends on model
            max_tokens: Some(4096), // LM Studio default - varies by model but 4096 is common default
            available_models: vec![
                "google/gemma-3-12b".to_string(),
                "google/gemma-3-27b".to_string(),
                "llama-3-70b".to_string(),
                "mistral-7b".to_string(),
                "tessa-rust-t1-7b".to_string(),
            ],
        }
    }
    
    fn provider_type(&self) -> ProviderType {
        ProviderType::LmStudio
    }
    
    fn supported_models(&self) -> Vec<&'static str> {
        vec![
            "google/gemma-3-12b",
            "google/gemma-3-27b",
            "llama-3-70b",
            "mistral-7b",
            "tessa-rust-t1-7b",
        ]
    }
    
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
}

impl LMStudioProvider {
    /// Convert Stood Messages format to OpenAI chat completion format
    fn convert_messages_to_openai(&self, messages: &Messages) -> Result<Vec<Value>, LlmError> {
        let mut openai_messages = Vec::new();
        
        for message in &messages.messages {
            let role = match message.role {
                MessageRole::User => "user",
                MessageRole::Assistant => "assistant",
                MessageRole::System => "system",
            };
            
            // Convert content blocks to OpenAI format (handle tool use and results)
            let mut content_parts = Vec::new();
            let mut tool_calls = Vec::new();
            let mut tool_results = Vec::new();
            
            for block in &message.content {
                match block {
                    ContentBlock::Text { text } => {
                        content_parts.push(text.clone());
                    }
                    ContentBlock::ToolUse { id, name, input } => {
                        // Convert tool use to OpenAI tool call format
                        // Ensure arguments is always a valid JSON object string
                        let arguments_str = if input.is_null() || !input.is_object() {
                            "{}".to_string()
                        } else {
                            serde_json::to_string(input).unwrap_or_else(|_| "{}".to_string())
                        };
                        tool_calls.push(serde_json::json!({
                            "id": id,
                            "type": "function",
                            "function": {
                                "name": name,
                                "arguments": arguments_str
                            }
                        }));
                    }
                    ContentBlock::ToolResult { tool_use_id, content: tool_content, .. } => {
                        // Convert tool result to OpenAI tool message format
                        // Tool results should always be converted to tool messages regardless of the containing message role
                        tool_results.push(serde_json::json!({
                            "role": "tool",
                            "tool_call_id": tool_use_id,
                            "content": tool_content.to_display_string()
                        }));
                    }
                    _ => {
                        // Skip other content types for now
                    }
                }
            }
            
            let content = content_parts.join(" ");
            
            if !content.is_empty() || !tool_calls.is_empty() {
                let mut message_json = serde_json::json!({
                    "role": role,
                    "content": if content.is_empty() { None } else { Some(content) }
                });
                
                // Add tool calls if present
                if !tool_calls.is_empty() {
                    message_json["tool_calls"] = serde_json::Value::Array(tool_calls);
                }
                
                openai_messages.push(message_json);
            }
            
            // Add tool results after the current message
            for tool_result in tool_results {
                openai_messages.push(tool_result);
            }
        }
        
        Ok(openai_messages)
    }
    
    /// Convert Stood Tool format to OpenAI tools format
    fn convert_tools_to_openai(&self, tools: &[Tool]) -> Result<Vec<serde_json::Value>, LlmError> {
        let openai_tools = tools.iter().map(|tool| {
            serde_json::json!({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.input_schema
                }
            })
        }).collect();
        Ok(openai_tools)
    }
    
    /// Convert OpenAI chat completion response to Stood ChatResponse format
    fn convert_openai_response_to_chat_response(&self, response: Value) -> Result<ChatResponse, LlmError> {
        // Extract the completion from OpenAI response format
        let choices = response.get("choices")
            .and_then(|c| c.as_array())
            .ok_or_else(|| LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: missing choices array".to_string(),
                source: None,
            })?;
        
        if choices.is_empty() {
            return Err(LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: empty choices array".to_string(),
                source: None,
            });
        }
        
        let first_choice = &choices[0];
        let message = first_choice.get("message")
            .ok_or_else(|| LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: missing message".to_string(),
                source: None,
            })?;
        
        let content = message.get("content")
            .and_then(|c| c.as_str())
            .ok_or_else(|| LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: missing content".to_string(),
                source: None,
            })?;
        
        // Extract usage information if available
        let usage = response.get("usage").and_then(|u| {
            Some(crate::llm::traits::Usage {
                input_tokens: u.get("prompt_tokens")?.as_u64()? as u32,
                output_tokens: u.get("completion_tokens")?.as_u64()? as u32,
                total_tokens: u.get("total_tokens")?.as_u64()? as u32,
            })
        });
        
        // Create metadata with finish reason
        let mut metadata = std::collections::HashMap::new();
        if let Some(finish_reason) = first_choice.get("finish_reason").and_then(|r| r.as_str()) {
            metadata.insert("finish_reason".to_string(), serde_json::Value::String(finish_reason.to_string()));
        }
        
        Ok(ChatResponse {
            content: content.to_string(),
            tool_calls: Vec::new(), // LM Studio typically doesn't support function calling
            thinking: None, // LM Studio doesn't have thinking mode
            usage,
            metadata,
        })
    }
    
    /// Convert OpenAI response to ChatResponse with tool calling support
    fn convert_openai_response_to_chat_response_with_tools(&self, response: serde_json::Value) -> Result<ChatResponse, LlmError> {
        let choices = response.get("choices")
            .and_then(|c| c.as_array())
            .ok_or_else(|| LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: missing choices array".to_string(),
                source: None,
            })?;
        
        if choices.is_empty() {
            return Err(LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: empty choices array".to_string(),
                source: None,
            });
        }
        
        let first_choice = &choices[0];
        let message = first_choice.get("message")
            .ok_or_else(|| LlmError::ProviderError {
                provider: ProviderType::LmStudio,
                message: "Invalid response format: missing message".to_string(),
                source: None,
            })?;
        
        // Extract content (might be null if only tool calls are present)
        let content = message.get("content")
            .and_then(|c| c.as_str())
            .unwrap_or("")
            .to_string();
        
        // Extract tool calls if present
        let mut tool_calls = Vec::new();
        if let Some(calls) = message.get("tool_calls").and_then(|tc| tc.as_array()) {
            for call in calls {
                if let (Some(id), Some(function)) = (
                    call.get("id").and_then(|i| i.as_str()),
                    call.get("function")
                ) {
                    if let Some(name) = function.get("name").and_then(|n| n.as_str()) {
                        // Handle arguments - they might be a string (JSON) or already parsed object
                        let parsed_args: serde_json::Value = match function.get("arguments") {
                            Some(serde_json::Value::String(s)) => {
                                // Arguments came as JSON string - try to parse it
                                serde_json::from_str(s).unwrap_or_else(|parse_err| {
                                    tracing::warn!("Failed to parse tool arguments as JSON: {} | Raw: {}", parse_err, s);
                                    // If parsing fails, return the string as-is
                                    serde_json::Value::String(s.clone())
                                })
                            },
                            Some(obj) => {
                                // Arguments already parsed as object
                                obj.clone()
                            },
                            None => serde_json::Value::Object(serde_json::Map::new()),
                        };
                        
                        tool_calls.push(crate::llm::traits::ToolCall {
                            id: id.to_string(),
                            name: name.to_string(),
                            input: parsed_args,
                        });
                    }
                }
            }
        }
        
        // Extract usage information if available
        let usage = response.get("usage").and_then(|u| {
            Some(crate::llm::traits::Usage {
                input_tokens: u.get("prompt_tokens")?.as_u64()? as u32,
                output_tokens: u.get("completion_tokens")?.as_u64()? as u32,
                total_tokens: u.get("total_tokens")?.as_u64()? as u32,
            })
        });
        
        // Create metadata with finish reason
        let mut metadata = std::collections::HashMap::new();
        if let Some(finish_reason) = first_choice.get("finish_reason").and_then(|r| r.as_str()) {
            metadata.insert("finish_reason".to_string(), serde_json::Value::String(finish_reason.to_string()));
        }
        
        Ok(ChatResponse {
            content,
            tool_calls,
            thinking: None, // LM Studio doesn't have thinking mode
            usage,
            metadata,
        })
    }
    
    async fn parse_sse_stream(
        &self,
        response: reqwest::Response,
    ) -> Result<Box<dyn Stream<Item = crate::llm::traits::StreamEvent> + Send + Unpin>, LlmError> {
        use futures::stream::{StreamExt, TryStreamExt};
        use crate::llm::traits::StreamEvent;
        
        let byte_stream = response.bytes_stream()
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e));
        
        let sse_stream = async_stream::stream! {
            let mut buffer = String::new();
            let mut lines_stream = byte_stream.map(|chunk_result| {
                chunk_result.map(|bytes| String::from_utf8_lossy(&bytes).to_string())
            });
            
            // Claude-style stateful tool management
            let mut tool_state = LMStudioToolState::new();
            
            // Track content for token estimation
            let mut total_content = String::new();
            
            tracing::debug!("üåä Starting SSE stream processing with stateful tool management...");
            let mut chunk_count = 0;
            let mut event_count = 0;
            
            while let Some(chunk_result) = lines_stream.next().await {
                match chunk_result {
                    Ok(chunk) => {
                        chunk_count += 1;
                        tracing::trace!("üåä Received HTTP chunk #{}: {} bytes: '{}'", 
                            chunk_count, chunk.len(), chunk.chars().take(100).collect::<String>());
                        
                        buffer.push_str(&chunk);
                        tracing::trace!("üåä Current buffer size: {} bytes", buffer.len());
                        
                        // Process complete lines
                        while let Some(newline_pos) = buffer.find('\n') {
                            let line = buffer[..newline_pos].trim().to_string();
                            buffer = buffer[newline_pos + 1..].to_string();
                            
                            tracing::trace!("üåä Processing SSE line: '{}'", line);
                            
                            // Parse SSE line with stateful tool management
                            if let Some(events) = Self::parse_sse_line_with_state(&line, &mut tool_state) {
                                for event in events {
                                    event_count += 1;
                                    
                                    // Track text content for token estimation
                                    if let StreamEvent::ContentBlockDelta { delta, .. } = &event {
                                        if let crate::llm::traits::ContentBlockDelta::Text { text } = delta {
                                            total_content.push_str(text);
                                        }
                                    }
                                    
                                    tracing::debug!("üåä Yielding event #{}: {:?}", event_count, 
                                        match &event {
                                            StreamEvent::ContentBlockDelta { delta, .. } => {
                                                match delta {
                                                    crate::llm::traits::ContentBlockDelta::Text { text } => format!("ContentBlockDelta::Text('{}')", text),
                                                    crate::llm::traits::ContentBlockDelta::ToolUse { tool_call_id, input_delta } => format!("ContentBlockDelta::ToolUse({}:'{}')", tool_call_id, input_delta),
                                                    _ => "ContentBlockDelta::Other".to_string(),
                                                }
                                            },
                                            StreamEvent::ContentBlockStart { block_type, .. } => format!("ContentBlockStart({:?})", block_type),
                                            StreamEvent::ContentBlockStop { .. } => "ContentBlockStop".to_string(),
                                            StreamEvent::MessageStop { .. } => "MessageStop".to_string(),
                                            StreamEvent::Error { error } => format!("Error({})", error),
                                            _ => "Other".to_string(),
                                        }
                                    );
                                    yield event;
                                }
                            } else {
                                tracing::trace!("üåä No events parsed from line: '{}'", line);
                            }
                        }
                    }
                    Err(e) => {
                        tracing::error!("‚ùå SSE stream error: {}", e);
                        yield StreamEvent::Error {
                            error: format!("Stream error: {}", e),
                        };
                        break;
                    }
                }
            }
            
            tracing::debug!("üåä SSE stream completed - processed {} chunks, yielded {} events", chunk_count, event_count);
            
            // If we haven't sent a Done event yet, send one now (handles cases where [DONE] is missing)
            if event_count > 0 {
                tracing::debug!("üåä Sending final MessageStop event since stream ended");
                
                // Estimate token usage based on content length (approximation)
                // Typical ratio is ~4 characters per token for English text
                let output_tokens = (total_content.len() / 4).max(1) as u32;
                let input_tokens = 100; // Rough estimate for input - this could be improved
                
                let usage = Some(crate::llm::traits::Usage {
                    input_tokens,
                    output_tokens,
                    total_tokens: input_tokens + output_tokens,
                });
                
                tracing::debug!("üåä LM Studio estimated token usage: input={}, output={}, total={}", 
                              input_tokens, output_tokens, input_tokens + output_tokens);
                
                yield StreamEvent::Done { usage };
            }
        };
        
        // Use boxed() to make the stream Unpin
        Ok(Box::new(sse_stream.boxed()))
    }
    
    
    /// Parse SSE line with stateful tool management (following Claude's pattern)
    fn parse_sse_line_with_state(
        line: &str, 
        tool_state: &mut LMStudioToolState
    ) -> Option<Vec<crate::llm::traits::StreamEvent>> {
        use crate::llm::traits::{StreamEvent, Usage, ContentBlockType, ContentBlockDelta};
        
        let mut events = Vec::new();
        
        // SSE format: "data: {json}"
        if let Some(data) = line.strip_prefix("data: ") {
            // Debug log the raw SSE data
            if tracing::level_enabled!(tracing::Level::DEBUG) {
                tracing::debug!("üîßüåä LM Studio SSE data: {}", data);
            }
            
            // JSON ONLY output for debug_json_only example
            if std::env::var("RUST_LOG").unwrap_or_default().contains("debug_json_only") && data != "[DONE]" {
                println!("IN {}", data);
            }
            
            if data.trim() == "[DONE]" {
                tracing::debug!("üåä Received [DONE] marker - finalizing any remaining tools");
                
                // Finalize any remaining tool calls like Claude does at content_block_stop
                let complete_tools = tool_state.get_all_complete_tools();
                for tool_call in complete_tools {
                    tracing::debug!("üîßüåä Finalizing tool call at stream end: {} with input: {}", 
                        tool_call.name, serde_json::to_string(&tool_call.input).unwrap_or_default());
                    
                    // Emit ContentBlockStart for tool use (like Claude)
                    events.push(StreamEvent::ContentBlockStart {
                        block_type: ContentBlockType::ToolUse,
                        block_index: 0,
                    });
                    
                    // Emit complete tool call (like Claude at content_block_stop)
                    events.push(StreamEvent::ToolCallStart {
                        tool_call: tool_call.clone(),
                    });
                    
                    // Emit complete input as delta for compatibility
                    if !tool_call.input.is_null() {
                        let input_str = serde_json::to_string(&tool_call.input).unwrap_or_default();
                        events.push(StreamEvent::ContentBlockDelta {
                            delta: ContentBlockDelta::ToolUse {
                                tool_call_id: tool_call.id.clone(),
                                input_delta: input_str,
                            },
                            block_index: 0,
                        });
                    }
                    
                    // Emit ContentBlockStop (like Claude)
                    events.push(StreamEvent::ContentBlockStop {
                        block_index: 0,
                    });
                }
                
                // Stream ending - don't send MessageStop, let main stream handle Done event
                // events.push(StreamEvent::MessageStop { 
                //     stop_reason: Some("end_turn".to_string()) 
                // });
                return Some(events);
            }
            
            // Parse JSON chunk
            match serde_json::from_str::<serde_json::Value>(data) {
                Ok(json) => {
                    tracing::trace!("üåä Parsed SSE chunk: {}", serde_json::to_string(&json).unwrap_or_default());
                    
                    // Extract delta content from OpenAI streaming format
                    if let Some(choices) = json.get("choices").and_then(|c| c.as_array()) {
                        if let Some(choice) = choices.first() {
                            if let Some(delta) = choice.get("delta") {
                                // Handle multiple tool calls in delta (fix major bug)
                                if let Some(tool_calls) = delta.get("tool_calls").and_then(|tc| tc.as_array()) {
                                    tracing::trace!("üîßüåä Found {} tool calls in delta", tool_calls.len());
                                    
                                    // Process ALL tool calls, not just the first one (major fix)
                                    for tool_call in tool_calls {
                                        let tool_index = tool_call.get("index").and_then(|i| i.as_u64()).unwrap_or(0);
                                        let tool_id = if let Some(actual_id) = tool_call.get("id").and_then(|i| i.as_str()) {
                                            actual_id.to_string()
                                        } else {
                                            format!("tool_call_{}", tool_index)
                                        };
                                        
                                        if let Some(function) = tool_call.get("function") {
                                            // Tool call start - has function name
                                            if let Some(name) = function.get("name").and_then(|n| n.as_str()) {
                                                tracing::debug!("üîßüåä Starting tool call: {} ({})", name, tool_id);
                                                tool_state.start_tool_call(tool_id.clone(), name.to_string());
                                                
                                                // Emit ContentBlockStart for tool use (like Claude)
                                                events.push(StreamEvent::ContentBlockStart {
                                                    block_type: ContentBlockType::ToolUse,
                                                    block_index: tool_index as usize,
                                                });
                                            }
                                            
                                            // Tool call delta - function arguments
                                            if let Some(arguments) = function.get("arguments").and_then(|args| args.as_str()) {
                                                if !arguments.is_empty() {
                                                    tracing::debug!("üîßüåä Adding input delta for {}: '{}'", tool_id, arguments);
                                                    tool_state.add_input_delta(&tool_id, arguments);
                                                    
                                                    // Emit ContentBlockDelta for tool use (like Claude)
                                                    events.push(StreamEvent::ContentBlockDelta {
                                                        delta: ContentBlockDelta::ToolUse {
                                                            tool_call_id: tool_id,
                                                            input_delta: arguments.to_string(),
                                                        },
                                                        block_index: tool_index as usize,
                                                    });
                                                }
                                            }
                                        }
                                    }
                                }
                                
                                // Handle regular content delta
                                if let Some(content) = delta.get("content").and_then(|c| c.as_str()) {
                                    if !content.is_empty() {
                                        tracing::trace!("üåä Content delta: '{}'", content);
                                        events.push(StreamEvent::ContentBlockDelta {
                                            delta: ContentBlockDelta::Text { 
                                                text: content.to_string() 
                                            },
                                            block_index: 0,
                                        });
                                    }
                                }
                            }
                            
                            // Handle finish_reason (finalize tools like Claude's content_block_stop)
                            if let Some(finish_reason) = choice.get("finish_reason") {
                                if !finish_reason.is_null() {
                                    let reason_str = finish_reason.as_str().unwrap_or("unknown");
                                    tracing::debug!("üåä Finish reason: {} - finalizing tools", reason_str);
                                    
                                    // Finalize all complete tool calls (like Claude at content_block_stop)
                                    let complete_tools = tool_state.get_all_complete_tools();
                                    for tool_call in complete_tools {
                                        tracing::info!("üîßüåä Finalizing complete tool call: {} with input: {}", 
                                            tool_call.name, serde_json::to_string(&tool_call.input).unwrap_or_default());
                                        
                                        // Emit complete tool call (like Claude)
                                        events.push(StreamEvent::ToolCallStart {
                                            tool_call: tool_call.clone(),
                                        });
                                        
                                        // Emit ContentBlockStop for this tool (like Claude)
                                        events.push(StreamEvent::ContentBlockStop {
                                            block_index: 0,
                                        });
                                    }
                                    
                                    // Stream ending - don't send MessageStop, let main stream handle Done event
                                    // events.push(StreamEvent::MessageStop {
                                    //     stop_reason: Some(reason_str.to_string()),
                                    // });
                                }
                            }
                        }
                    }
                    
                    // Handle usage information - only emit metadata if we have valid token data
                    if let Some(usage_data) = json.get("usage") {
                        let input_tokens = usage_data.get("prompt_tokens").and_then(|t| t.as_u64()).unwrap_or(0) as u32;
                        let output_tokens = usage_data.get("completion_tokens").and_then(|t| t.as_u64()).unwrap_or(0) as u32;
                        let total_tokens = usage_data.get("total_tokens").and_then(|t| t.as_u64()).unwrap_or(0) as u32;
                        
                        // Only emit metadata if we have valid (non-zero) token data
                        // This allows fallback estimation to work when LM Studio doesn't provide usage data
                        if input_tokens > 0 || output_tokens > 0 || total_tokens > 0 {
                            let usage = Usage {
                                input_tokens,
                                output_tokens,
                                total_tokens,
                            };
                            
                            tracing::debug!("üåä Usage metadata: {:?}", usage);
                            events.push(StreamEvent::Metadata { usage: Some(usage) });
                        } else {
                            tracing::debug!("üåä Skipping zero-token usage metadata, allowing fallback estimation");
                        }
                    }
                }
                Err(e) => {
                    tracing::warn!("‚ö†Ô∏è Failed to parse SSE JSON: {} | Data: {}", e, data);
                    // Convert JSON parse errors to StreamEvent::Error to properly handle 404 and other errors
                    events.push(StreamEvent::Error {
                        error: format!("Failed to parse SSE response: {} | Data: {}", e, data),
                    });
                }
            }
        }
        
        if events.is_empty() { None } else { Some(events) }
    }
}